{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34b3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import dycomutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import neptune\n",
    "import pandas as pd # Note: pandas is not needed for this class\n",
    "import sys\n",
    "sys.path.append(\"/home/desild/work/academic/sem3/TrustworthyML-assignment/tacotron2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254a15e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/Botz/Audio-MI/e/AUD1-509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'tuple'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs-legacy.neptune.ai/help/value_of_unsupported_type\n",
      "[neptune] [warning] NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs-legacy.neptune.ai/help/value_of_unsupported_type\n"
     ]
    }
   ],
   "source": [
    "MAIN_DIR = \"/mnt/i/My Drive/TrustworthyML-assignment\"\n",
    "\n",
    "# You can just define the dictionary directly\n",
    "opt = {\n",
    "    \"n_epochs\": 4000,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.0002,\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.999,\n",
    "    \"n_cpu\": 8,\n",
    "    \"latent_dim\": 128,\n",
    "    \"img_size\": (1,80,16),  \n",
    "    \"sample_interval\": 400,\n",
    "    \"gamma\": 0.75,\n",
    "    \"lambda_k\": 0.001,\n",
    "    \"lambda_gp\": 10,\n",
    "    \"load_gen\": None, #os.path.join(MAIN_DIR, \"Code\", \"saved_models\", \"generator.pth\"),\n",
    "    \"load_dis\": None, #os.path.join(MAIN_DIR, \"Code\", \"saved_models\", \"discriminator.pth\"),\n",
    "}\n",
    "\n",
    "load_dotenv(os.path.join(MAIN_DIR, \"Code\", \".ENV\"))\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"Botz/Audio-MI\",\n",
    "    name=\"hifigan-training\",\n",
    "    api_token=os.getenv(\"NEPTUNE_API_TOKEN\")\n",
    ")\n",
    "run[\"parameters\"] = opt\n",
    "\n",
    "opt = dycomutils.config.ConfigDict(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e95bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = (80//16, 16 // 16)\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 64 * self.init_size[0] * self.init_size[1]))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.l1(noise)\n",
    "        out = out.view(out.shape[0], 64, self.init_size[0], self.init_size[1])\n",
    "        img = self.conv_blocks(out)\n",
    "        return img.squeeze(1)\n",
    "    \n",
    "class ImgDiscriminator(nn.Module):\n",
    "    def __init__(self, img_size=opt.img_size):\n",
    "        super(ImgDiscriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_size)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.reshape(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fe5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def flip(x, dim):\n",
    "    xsize = x.size()\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    x = x.contiguous()\n",
    "    x = x.view(-1, *xsize[dim:])\n",
    "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n",
    "                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
    "    return x.view(xsize)\n",
    "\n",
    "\n",
    "def sinc(band,t_right):\n",
    "    y_right= torch.sin(2*math.pi*band*t_right)/(2*math.pi*band*t_right)\n",
    "    y_left= flip(y_right,0)\n",
    "\n",
    "    y=torch.cat([y_left,Variable(torch.ones(1)).cuda(),y_right])\n",
    "\n",
    "    return y\n",
    "    \n",
    "\n",
    "class SincConv_fast(nn.Module):\n",
    "    \"\"\"Sinc-based convolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : `int`\n",
    "        Number of input channels. Must be 1.\n",
    "    out_channels : `int`\n",
    "        Number of filters.\n",
    "    kernel_size : `int`\n",
    "        Filter length.\n",
    "    sample_rate : `int`, optional\n",
    "        Sample rate. Defaults to 16000.\n",
    "    Usage\n",
    "    -----\n",
    "    See `torch.nn.Conv1d`\n",
    "    Reference\n",
    "    ---------\n",
    "    Mirco Ravanelli, Yoshua Bengio,\n",
    "    \"Speaker Recognition from raw waveform with SincNet\".\n",
    "    https://arxiv.org/abs/1808.00158\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\n",
    "\n",
    "        super(SincConv_fast,self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "            #msg = (f'SincConv only support one input channel '\n",
    "            #       f'(here, in_channels = {in_channels:d}).')\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
    "        if kernel_size%2==0:\n",
    "            self.kernel_size=self.kernel_size+1\n",
    "            \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if bias:\n",
    "            raise ValueError('SincConv does not support bias.')\n",
    "        if groups > 1:\n",
    "            raise ValueError('SincConv does not support groups.')\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\n",
    "        low_hz = 30\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "\n",
    "        mel = np.linspace(self.to_mel(low_hz),\n",
    "                          self.to_mel(high_hz),\n",
    "                          self.out_channels + 1)\n",
    "        hz = self.to_hz(mel)\n",
    "        \n",
    "\n",
    "        # filter lower frequency (out_channels, 1)\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
    "\n",
    "        # filter frequency band (out_channels, 1)\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
    "\n",
    "        # Hamming window\n",
    "        #self.window_ = torch.hamming_window(self.kernel_size)\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
    "\n",
    "\n",
    "        # (1, kernel_size/2)\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n",
    "            Batch of waveforms.\n",
    "        Returns\n",
    "        -------\n",
    "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n",
    "            Batch of sinc filters activations.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_ = self.n_.to(waveforms.device)\n",
    "\n",
    "        self.window_ = self.window_.to(waveforms.device)\n",
    "\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
    "        \n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
    "        band=(high-low)[:,0]\n",
    "        \n",
    "        f_times_t_low = torch.matmul(low, self.n_)\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\n",
    "\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n",
    "        band_pass_center = 2*band.view(-1,1)\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
    "        \n",
    "        \n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
    "\n",
    "        \n",
    "        band_pass = band_pass / (2*band[:,None])\n",
    "        \n",
    "\n",
    "        self.filters = (band_pass).view(\n",
    "            self.out_channels, 1, self.kernel_size)\n",
    "\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride,\n",
    "                        padding=self.padding, dilation=self.dilation,\n",
    "                         bias=None, groups=1) \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class sinc_conv(nn.Module):\n",
    "\n",
    "    def __init__(self, N_filt,Filt_dim,fs):\n",
    "        super(sinc_conv,self).__init__()\n",
    "\n",
    "        # Mel Initialization of the filterbanks\n",
    "        low_freq_mel = 80\n",
    "        high_freq_mel = (2595 * np.log10(1 + (fs / 2) / 700))  # Convert Hz to Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, N_filt)  # Equally spaced in Mel scale\n",
    "        f_cos = (700 * (10**(mel_points / 2595) - 1)) # Convert Mel to Hz\n",
    "        b1=np.roll(f_cos,1)\n",
    "        b2=np.roll(f_cos,-1)\n",
    "        b1[0]=30\n",
    "        b2[-1]=(fs/2)-100\n",
    "                \n",
    "        self.freq_scale=fs*1.0\n",
    "        self.filt_b1 = nn.Parameter(torch.from_numpy(b1/self.freq_scale))\n",
    "        self.filt_band = nn.Parameter(torch.from_numpy((b2-b1)/self.freq_scale))\n",
    "\n",
    "        \n",
    "        self.N_filt=N_filt\n",
    "        self.Filt_dim=Filt_dim\n",
    "        self.fs=fs\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        filters=Variable(torch.zeros((self.N_filt,self.Filt_dim))).cuda()\n",
    "        N=self.Filt_dim\n",
    "        t_right=Variable(torch.linspace(1, (N-1)/2, steps=int((N-1)/2))/self.fs).cuda()\n",
    "        \n",
    "        \n",
    "        min_freq=50.0;\n",
    "        min_band=50.0;\n",
    "        \n",
    "        filt_beg_freq=torch.abs(self.filt_b1)+min_freq/self.freq_scale\n",
    "        filt_end_freq=filt_beg_freq+(torch.abs(self.filt_band)+min_band/self.freq_scale)\n",
    "       \n",
    "        n=torch.linspace(0, N, steps=N)\n",
    "\n",
    "        # Filter window (hamming)\n",
    "        window=0.54-0.46*torch.cos(2*math.pi*n/N);\n",
    "        window=Variable(window.float().cuda())\n",
    "\n",
    "        \n",
    "        for i in range(self.N_filt):\n",
    "                        \n",
    "            low_pass1 = 2*filt_beg_freq[i].float()*sinc(filt_beg_freq[i].float()*self.freq_scale,t_right)\n",
    "            low_pass2 = 2*filt_end_freq[i].float()*sinc(filt_end_freq[i].float()*self.freq_scale,t_right)\n",
    "            band_pass=(low_pass2-low_pass1)\n",
    "\n",
    "            band_pass=band_pass/torch.max(band_pass)\n",
    "\n",
    "            filters[i,:]=band_pass.cuda()*window\n",
    "\n",
    "        out=F.conv1d(x, filters.view(self.N_filt,1,self.Filt_dim))\n",
    "    \n",
    "        return out\n",
    "    \n",
    "\n",
    "def act_fun(act_type):\n",
    "\n",
    " if act_type==\"relu\":\n",
    "    return nn.ReLU()\n",
    "            \n",
    " if act_type==\"tanh\":\n",
    "    return nn.Tanh()\n",
    "            \n",
    " if act_type==\"sigmoid\":\n",
    "    return nn.Sigmoid()\n",
    "           \n",
    " if act_type==\"leaky_relu\":\n",
    "    return nn.LeakyReLU(0.2)\n",
    "            \n",
    " if act_type==\"elu\":\n",
    "    return nn.ELU()\n",
    "                     \n",
    " if act_type==\"softmax\":\n",
    "    return nn.LogSoftmax(dim=1)\n",
    "        \n",
    " if act_type==\"linear\":\n",
    "    return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n",
    "            \n",
    "            \n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, options):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_dim=int(options['input_dim'])\n",
    "        self.fc_lay=options['fc_lay']\n",
    "        self.fc_drop=options['fc_drop']\n",
    "        self.fc_use_batchnorm=options['fc_use_batchnorm']\n",
    "        self.fc_use_laynorm=options['fc_use_laynorm']\n",
    "        self.fc_use_laynorm_inp=options['fc_use_laynorm_inp']\n",
    "        self.fc_use_batchnorm_inp=options['fc_use_batchnorm_inp']\n",
    "        self.fc_act=options['fc_act']\n",
    "        \n",
    "       \n",
    "        self.wx  = nn.ModuleList([])\n",
    "        self.bn  = nn.ModuleList([])\n",
    "        self.ln  = nn.ModuleList([])\n",
    "        self.act = nn.ModuleList([])\n",
    "        self.drop = nn.ModuleList([])\n",
    "       \n",
    "\n",
    "       \n",
    "        # input layer normalization\n",
    "        if self.fc_use_laynorm_inp:\n",
    "           self.ln0=LayerNorm(self.input_dim)\n",
    "          \n",
    "        # input batch normalization    \n",
    "        if self.fc_use_batchnorm_inp:\n",
    "           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n",
    "           \n",
    "           \n",
    "        self.N_fc_lay=len(self.fc_lay)\n",
    "             \n",
    "        current_input=self.input_dim\n",
    "        \n",
    "        # Initialization of hidden layers\n",
    "        \n",
    "        for i in range(self.N_fc_lay):\n",
    "            \n",
    "         # dropout\n",
    "         self.drop.append(nn.Dropout(p=self.fc_drop[i]))\n",
    "         \n",
    "         # activation\n",
    "         self.act.append(act_fun(self.fc_act[i]))\n",
    "         \n",
    "         \n",
    "         add_bias=True\n",
    "         \n",
    "         # layer norm initialization\n",
    "         self.ln.append(LayerNorm(self.fc_lay[i]))\n",
    "         self.bn.append(nn.BatchNorm1d(self.fc_lay[i],momentum=0.05))\n",
    "         \n",
    "         if self.fc_use_laynorm[i] or self.fc_use_batchnorm[i]:\n",
    "             add_bias=False\n",
    "         \n",
    "              \n",
    "         # Linear operations\n",
    "         self.wx.append(nn.Linear(current_input, self.fc_lay[i],bias=add_bias))\n",
    "         \n",
    "         # weight initialization\n",
    "         self.wx[i].weight = torch.nn.Parameter(torch.Tensor(self.fc_lay[i],current_input).uniform_(-np.sqrt(0.01/(current_input+self.fc_lay[i])),np.sqrt(0.01/(current_input+self.fc_lay[i]))))\n",
    "         self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.fc_lay[i]))\n",
    "         \n",
    "         current_input=self.fc_lay[i]\n",
    "         \n",
    "         \n",
    "    def forward(self, x):\n",
    "        \n",
    "      # Applying Layer/Batch Norm\n",
    "      if bool(self.fc_use_laynorm_inp):\n",
    "        x=self.ln0((x))\n",
    "        \n",
    "      if bool(self.fc_use_batchnorm_inp):\n",
    "        x=self.bn0((x))\n",
    "        \n",
    "      for i in range(self.N_fc_lay):\n",
    "\n",
    "        if self.fc_act[i]!='linear':\n",
    "            \n",
    "          if self.fc_use_laynorm[i]:\n",
    "           x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))\n",
    "          \n",
    "          if self.fc_use_batchnorm[i]:\n",
    "           x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))\n",
    "          \n",
    "          if self.fc_use_batchnorm[i]==False and self.fc_use_laynorm[i]==False:\n",
    "           x = self.drop[i](self.act[i](self.wx[i](x)))\n",
    "           \n",
    "        else:\n",
    "          if self.fc_use_laynorm[i]:\n",
    "           x = self.drop[i](self.ln[i](self.wx[i](x)))\n",
    "          \n",
    "          if self.fc_use_batchnorm[i]:\n",
    "           x = self.drop[i](self.bn[i](self.wx[i](x)))\n",
    "          \n",
    "          if self.fc_use_batchnorm[i]==False and self.fc_use_laynorm[i]==False:\n",
    "           x = self.drop[i](self.wx[i](x)) \n",
    "          \n",
    "      return x\n",
    "\n",
    "\n",
    "\n",
    "class SincNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,options):\n",
    "       super(SincNet,self).__init__()\n",
    "    \n",
    "       self.cnn_N_filt=options['cnn_N_filt']\n",
    "       self.cnn_len_filt=options['cnn_len_filt']\n",
    "       self.cnn_max_pool_len=options['cnn_max_pool_len']\n",
    "       \n",
    "       \n",
    "       self.cnn_act=options['cnn_act']\n",
    "       self.cnn_drop=options['cnn_drop']\n",
    "       \n",
    "       self.cnn_use_laynorm=options['cnn_use_laynorm']\n",
    "       self.cnn_use_batchnorm=options['cnn_use_batchnorm']\n",
    "       self.cnn_use_laynorm_inp=options['cnn_use_laynorm_inp']\n",
    "       self.cnn_use_batchnorm_inp=options['cnn_use_batchnorm_inp']\n",
    "       \n",
    "       self.input_dim=int(options['input_dim'])\n",
    "       \n",
    "       self.fs=options['fs']\n",
    "       \n",
    "       self.N_cnn_lay=len(options['cnn_N_filt'])\n",
    "       self.conv  = nn.ModuleList([])\n",
    "       self.bn  = nn.ModuleList([])\n",
    "       self.ln  = nn.ModuleList([])\n",
    "       self.act = nn.ModuleList([])\n",
    "       self.drop = nn.ModuleList([])\n",
    "       \n",
    "             \n",
    "       if self.cnn_use_laynorm_inp:\n",
    "           self.ln0=LayerNorm(self.input_dim)\n",
    "           \n",
    "       if self.cnn_use_batchnorm_inp:\n",
    "           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n",
    "           \n",
    "       current_input=self.input_dim \n",
    "       \n",
    "       for i in range(self.N_cnn_lay):\n",
    "         \n",
    "         N_filt=int(self.cnn_N_filt[i])\n",
    "         len_filt=int(self.cnn_len_filt[i])\n",
    "         \n",
    "         # dropout\n",
    "         self.drop.append(nn.Dropout(p=self.cnn_drop[i]))\n",
    "         \n",
    "         # activation\n",
    "         self.act.append(act_fun(self.cnn_act[i]))\n",
    "                    \n",
    "         # layer norm initialization         \n",
    "         self.ln.append(LayerNorm([N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])]))\n",
    "\n",
    "         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i]),momentum=0.05))\n",
    "            \n",
    "\n",
    "         if i==0:\n",
    "          self.conv.append(SincConv_fast(self.cnn_N_filt[0],self.cnn_len_filt[0],self.fs))\n",
    "              \n",
    "         else:\n",
    "          self.conv.append(nn.Conv1d(self.cnn_N_filt[i-1], self.cnn_N_filt[i], self.cnn_len_filt[i]))\n",
    "          \n",
    "         current_input=int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])\n",
    "\n",
    "         \n",
    "       self.out_dim=current_input*N_filt\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "       batch=x.shape[0]\n",
    "       seq_len=x.shape[1]\n",
    "       \n",
    "       if bool(self.cnn_use_laynorm_inp):\n",
    "        x=self.ln0((x))\n",
    "        \n",
    "       if bool(self.cnn_use_batchnorm_inp):\n",
    "        x=self.bn0((x))\n",
    "        \n",
    "       x=x.view(batch,1,seq_len)\n",
    "\n",
    "       \n",
    "       for i in range(self.N_cnn_lay):\n",
    "           \n",
    "         if self.cnn_use_laynorm[i]:\n",
    "          if i==0:\n",
    "           x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(torch.abs(self.conv[i](x)), self.cnn_max_pool_len[i]))))  \n",
    "          else:\n",
    "           x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))   \n",
    "          \n",
    "         if self.cnn_use_batchnorm[i]:\n",
    "          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n",
    "\n",
    "         if self.cnn_use_batchnorm[i]==False and self.cnn_use_laynorm[i]==False:\n",
    "          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n",
    "\n",
    "       \n",
    "       x = x.view(batch,-1)\n",
    "\n",
    "       return x\n",
    "   \n",
    "def str_to_bool(s):\n",
    "    if s == 'True':\n",
    "         return True\n",
    "    elif s == 'False':\n",
    "         return False\n",
    "    else:\n",
    "         raise ValueError \n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "# %%\n",
    "RESAMPLE_RATE = 16000\n",
    "\n",
    "fs=f\"{RESAMPLE_RATE}\"\n",
    "cw_len=\"1024\"\n",
    "cw_shift=\"10\"   \n",
    "\n",
    "cnn_N_filt=\"100,80,80\"\n",
    "cnn_len_filt=\"251,5,5\"\n",
    "cnn_max_pool_len=\"3,3,3\"\n",
    "cnn_use_laynorm_inp=\"True\"\n",
    "cnn_use_batchnorm_inp=\"False\"\n",
    "cnn_use_laynorm=\"True,True,True\"\n",
    "cnn_use_batchnorm=\"False,False,False\"\n",
    "cnn_act=\"leaky_relu,leaky_relu,leaky_relu\"\n",
    "cnn_drop=\"0.1,0.1,0.1\"\n",
    "\n",
    "\n",
    "fc_lay=\"2048,2048,2048\"\n",
    "fc_drop=\"0.1,0.1,0.1\"\n",
    "fc_use_laynorm_inp=\"True\"\n",
    "fc_use_batchnorm_inp=\"False\"\n",
    "fc_use_batchnorm=\"True,True,True\"\n",
    "fc_use_laynorm=\"False,False,False\"\n",
    "fc_act=\"leaky_relu,leaky_relu,leaky_relu\"\n",
    "\n",
    "class_lay=\"1\"\n",
    "class_drop=\"0.0\"\n",
    "class_use_laynorm_inp=\"False\"\n",
    "class_use_batchnorm_inp=\"False\"\n",
    "class_use_batchnorm=\"False\"\n",
    "class_use_laynorm=\"False\"\n",
    "class_act=\"softmax\"\n",
    "\n",
    "lr=\"0.0004\"\n",
    "batch_size=\"128\"\n",
    "N_epochs=\"1500\"\n",
    "N_batches=\"800\"\n",
    "N_eval_epoch=\"8\"\n",
    "seed=\"1234\"\n",
    "\n",
    "# %%\n",
    "cnn_N_filt=list(map(int, cnn_N_filt.split(',')))\n",
    "cnn_len_filt=list(map(int, cnn_len_filt.split(',')))\n",
    "cnn_max_pool_len=list(map(int, cnn_max_pool_len.split(',')))\n",
    "cnn_use_laynorm_inp=str_to_bool(cnn_use_laynorm_inp)\n",
    "cnn_use_batchnorm_inp=str_to_bool(cnn_use_batchnorm_inp)\n",
    "cnn_use_laynorm=list(map(str_to_bool, cnn_use_laynorm.split(',')))\n",
    "cnn_use_batchnorm=list(map(str_to_bool, cnn_use_batchnorm.split(',')))\n",
    "cnn_act=list(map(str, cnn_act.split(',')))\n",
    "cnn_drop=list(map(float, cnn_drop.split(',')))\n",
    "\n",
    "\n",
    "#[dnn]\n",
    "fc_lay=list(map(int, fc_lay.split(',')))\n",
    "fc_drop=list(map(float, fc_drop.split(',')))\n",
    "fc_use_laynorm_inp=str_to_bool(fc_use_laynorm_inp)\n",
    "fc_use_batchnorm_inp=str_to_bool(fc_use_batchnorm_inp)\n",
    "fc_use_batchnorm=list(map(str_to_bool, fc_use_batchnorm.split(',')))\n",
    "fc_use_laynorm=list(map(str_to_bool, fc_use_laynorm.split(',')))\n",
    "fc_act=list(map(str, fc_act.split(',')))\n",
    "\n",
    "#[class]\n",
    "class_lay=list(map(int, class_lay.split(',')))\n",
    "class_drop=list(map(float, class_drop.split(',')))\n",
    "class_use_laynorm_inp=str_to_bool(class_use_laynorm_inp)\n",
    "class_use_batchnorm_inp=str_to_bool(class_use_batchnorm_inp)\n",
    "class_use_batchnorm=list(map(str_to_bool, class_use_batchnorm.split(',')))\n",
    "class_use_laynorm=list(map(str_to_bool, class_use_laynorm.split(',')))\n",
    "class_act=list(map(str, class_act.split(',')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089a309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desild/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10960])\n",
      "10960\n",
      "torch.Size([2, 2048])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "wlen=4000\n",
    "\n",
    "\n",
    "# %%\n",
    "# Feature extractor CNN\n",
    "CNN_arch = {\n",
    "    'input_dim': wlen,\n",
    "    'fs': int(fs),\n",
    "    'cnn_N_filt': cnn_N_filt,\n",
    "    'cnn_len_filt': cnn_len_filt,\n",
    "    'cnn_max_pool_len':cnn_max_pool_len,\n",
    "    'cnn_use_laynorm_inp': cnn_use_laynorm_inp,\n",
    "    'cnn_use_batchnorm_inp': cnn_use_batchnorm_inp,\n",
    "    'cnn_use_laynorm':cnn_use_laynorm,\n",
    "    'cnn_use_batchnorm':cnn_use_batchnorm,\n",
    "    'cnn_act': cnn_act,\n",
    "    'cnn_drop':cnn_drop,          \n",
    "}\n",
    "\n",
    "\n",
    "CNN_net=SincNet(CNN_arch)\n",
    "CNN_net.cuda()\n",
    "\n",
    "\n",
    "\n",
    "DNN1_arch = {\n",
    "    'input_dim': CNN_net.out_dim,\n",
    "    'fc_lay': fc_lay,\n",
    "    'fc_drop': fc_drop, \n",
    "    'fc_use_batchnorm': fc_use_batchnorm,\n",
    "    'fc_use_laynorm': fc_use_laynorm,\n",
    "    'fc_use_laynorm_inp': fc_use_laynorm_inp,\n",
    "    'fc_use_batchnorm_inp':fc_use_batchnorm_inp,\n",
    "    'fc_act': fc_act,\n",
    "}\n",
    "\n",
    "DNN1_net=MLP(DNN1_arch)\n",
    "DNN1_net.cuda()\n",
    "\n",
    "\n",
    "DNN2_arch = {'input_dim':fc_lay[-1] ,\n",
    "          'fc_lay': class_lay,\n",
    "          'fc_drop': class_drop, \n",
    "          'fc_use_batchnorm': class_use_batchnorm,\n",
    "          'fc_use_laynorm': class_use_laynorm,\n",
    "          'fc_use_laynorm_inp': class_use_laynorm_inp,\n",
    "          'fc_use_batchnorm_inp':class_use_batchnorm_inp,\n",
    "          'fc_act': class_act,\n",
    "          }\n",
    "\n",
    "\n",
    "DNN2_net=MLP(DNN2_arch)\n",
    "DNN2_net.cuda()\n",
    "\n",
    "# %%\n",
    "inp = torch.randn(2,  wlen).cuda()\n",
    "out1 = CNN_net(inp)\n",
    "print(out1.shape)\n",
    "\n",
    "print(CNN_net.out_dim)\n",
    "out2 = DNN1_net(out1)\n",
    "print(out2.shape)\n",
    "\n",
    "# %%\n",
    "pout=DNN2_net(DNN1_net(CNN_net(inp)))\n",
    "print(pout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e198fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_net_sr=SincNet(CNN_arch)\n",
    "CNN_net_sr.cuda()\n",
    "DNN1_net_sr=MLP(DNN1_arch)\n",
    "DNN1_net_sr.cuda()\n",
    "\n",
    "DNN2_arch_sr = {'input_dim':fc_lay[-1] ,\n",
    "          'fc_lay': [35],\n",
    "          'fc_drop': class_drop, \n",
    "          'fc_use_batchnorm': class_use_batchnorm,\n",
    "          'fc_use_laynorm': class_use_laynorm,\n",
    "          'fc_use_laynorm_inp': class_use_laynorm_inp,\n",
    "          'fc_use_batchnorm_inp':class_use_batchnorm_inp,\n",
    "          'fc_act': class_act,\n",
    "          }\n",
    "\n",
    "\n",
    "DNN2_net_sr=MLP(DNN2_arch_sr)\n",
    "DNN2_net_sr.cuda()\n",
    "\n",
    "sr_weight = torch.load(\"/home/desild/work/academic/sem3/TrustworthyML-assignment/tacotron2/vctk/models/SINCNET_SR/20251129_142613/checkpoint.pth\", weights_only=False)\n",
    "CNN_net_sr.load_state_dict(sr_weight['CNN_net'])\n",
    "DNN1_net_sr.load_state_dict(sr_weight['DNN1_net'])\n",
    "DNN2_net_sr.load_state_dict(sr_weight['DNN2_net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2cdab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49875\n"
     ]
    }
   ],
   "source": [
    "from common_local.utils import load_wav_to_torch, load_filepaths_and_text, to_gpu\n",
    "from common_local.audio_processing import dynamic_range_compression, dynamic_range_decompression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import librosa\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "load_in_memory = {}\n",
    "\n",
    "class MelAudioLoaderVoxCeleb(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        1) loads audio,text pairs\n",
    "        2) computes mel-spectrograms from audio files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, main_file, filter_length, hop_length, win_length,\n",
    "                 n_mel_channels, sampling_rate, mel_fmin, mel_fmax, segment_length, max_wav_value, base_directory='..',\n",
    "                 filter_speakers: Optional[List[str]] = None):\n",
    "        \n",
    "        self.main_file = main_file\n",
    "        if filter_speakers is not None:\n",
    "            self.main_file = self.main_file[self.main_file['speaker_id'].isin(filter_speakers)]\n",
    "            print(f\"Filtered dataset to {len(self.main_file)} items for speakers: {filter_speakers}\")\n",
    "\n",
    "        self.max_wav_value = max_wav_value\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.base_directory = base_directory\n",
    "        self.random_amp = 0.2\n",
    "        \n",
    "        self.filter_length = filter_length\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.mel_fmin = mel_fmin\n",
    "        self.mel_fmax = mel_fmax\n",
    "        self.stats = {\n",
    "            \"audio_len_distribution\": {},\n",
    "        }\n",
    "\n",
    "        \n",
    "        self.speaker_id2id_map = {}\n",
    "        self.segment_length = segment_length\n",
    "        random.seed(1234)\n",
    "        self.main_file = self.main_file.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    def spectral_normalize(self, magnitudes):\n",
    "        output = dynamic_range_compression(magnitudes)\n",
    "        return output\n",
    "    \n",
    "    def load_text(self, file_path):\n",
    "        file_path = file_path.replace(\"WAV.wav\", \"WRD\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            text = f.read().strip()\n",
    "            \n",
    "        text = text.split('\\n')\n",
    "        text = [x.strip().split(' ', 2) for x in text if len(x.split(' ', 2)) == 3]\n",
    "        return [int(x[0]) for x in text], [int(x[1]) for x in text], [x[2] for x in text]\n",
    "    \n",
    "    def get_mel_audio_pair(self, filename):\n",
    "        if filename not in load_in_memory:\n",
    "            audio, sampling_rate = librosa.load(filename, sr=None)\n",
    "            load_in_memory[filename] = {}\n",
    "            load_in_memory[filename][\"audio\"] = (audio, sampling_rate)\n",
    "        else:\n",
    "            audio, sampling_rate = load_in_memory[filename][\"audio\"]\n",
    "\n",
    "        # if sampling_rate != self.sampling_rate:\n",
    "        #     raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "        #         sampling_rate, self.sampling_rate))\n",
    "        \n",
    "        #\n",
    "        # if RESAMPLE_RATE != self.sampling_rate:\n",
    "        #import soundfile as sf\n",
    "        #sf.write(\"before_resample.wav\", audio, sampling_rate)\n",
    "        audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=6000)\n",
    "        #sf.write(\"after_resample.wav\", audio, sampling_rate//8)\n",
    "        audio = torch.from_numpy(audio)\n",
    "\n",
    "        self.stats[\"audio_len_distribution\"][filename] = audio.size(0)\n",
    "        # Take segment\n",
    "        if audio.size(0) >= self.segment_length:\n",
    "            max_audio_start = audio.size(0) - self.segment_length\n",
    "            audio_start = random.randint(0, max_audio_start)\n",
    "            audio = audio[audio_start:audio_start+self.segment_length]\n",
    "        else:\n",
    "            audio = torch.nn.functional.pad(\n",
    "                audio, (0, self.segment_length - audio.size(0)), 'constant').data\n",
    "        \n",
    "        #sf.write(\"after_resample2.wav\", audio.numpy(), sampling_rate//8)\n",
    "        audio = audio * random.uniform(1.0 - self.random_amp, 1.0 + self.random_amp)\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio.numpy(),\n",
    "            sr=self.sampling_rate,\n",
    "            n_fft=self.filter_length,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            n_mels=self.n_mel_channels,\n",
    "            fmin=self.mel_fmin,\n",
    "            fmax=self.mel_fmax\n",
    "        )\n",
    "        \n",
    "        mel_spec = torch.from_numpy(mel_spec).unsqueeze(0)\n",
    "        mel_output = self.spectral_normalize(mel_spec)\n",
    "        mel_output = mel_output.squeeze(0)\n",
    "        return mel_output, audio_norm, audio, len(audio)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n",
    "        output = self.get_mel_audio_pair(self.main_file.iloc[index,4])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.main_file.shape[0]-1\n",
    "\n",
    "def batch_to_gpu(batch) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor, torch.Tensor, List[str]]: \n",
    "    x, y, len_y, text = batch\n",
    "    x = to_gpu(x).float()\n",
    "    y = to_gpu(y).float()\n",
    "    len_y = to_gpu(torch.sum(len_y))\n",
    "\n",
    "    return (x, y), y, len_y, text\n",
    "\n",
    "def get_text_embedding(text, model):\n",
    "    embeddings = model.encode([text])\n",
    "    #print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    if len(real_samples.shape) == 2:\n",
    "        alpha = torch.from_numpy(np.random.random((real_samples.size(0), 1)))\n",
    "    else:\n",
    "        alpha = torch.from_numpy(np.random.random((real_samples.size(0), 1, 1)))\n",
    "    alpha = alpha.to(real_samples.device).float()\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    #print(interpolates.shape, real_samples.shape, fake_samples.shape, alpha.shape)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(torch.from_numpy(np.ones((real_samples.shape[0], 1))).float(), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake.to(real_samples.device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# %%\n",
    "\n",
    "class Config:\n",
    "    # ** Audio params **\n",
    "    sampling_rate = 6000                        # Sampling rate\n",
    "    filter_length = 1024                         # Filter length\n",
    "    hop_length = 256                             # Hop (stride) length\n",
    "    win_length = 1024                            # Window length\n",
    "    mel_fmin = 0.0                               # Minimum mel frequency\n",
    "    mel_fmax = 8000.0                            # Maximum mel frequency\n",
    "    n_mel_channels = 80                          # Number of bins in mel-spectrograms\n",
    "    max_wav_value = 32768.0 \n",
    "    segment_length = 4000  \n",
    "    batch_size = 256  # Length of training segments (in samples)\n",
    "    epochs = 500\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "train_data = pd.read_csv(\"/home/desild/work/academic/sem3/TrustworthyML-assignment/data/raw/voxceleb/unseen_data.csv\")\n",
    "\n",
    "# loss function\n",
    "cost = nn.NLLLoss()\n",
    "\n",
    "# %%\n",
    "print(len(train_data))\n",
    "\n",
    "# %%\n",
    "train_ds = MelAudioLoaderVoxCeleb(train_data, \n",
    "                                  config.filter_length, \n",
    "                                  config.hop_length, \n",
    "                                  config.win_length,\n",
    "                                  config.n_mel_channels, \n",
    "                                  config.sampling_rate, \n",
    "                                  config.mel_fmin, \n",
    "                                  config.mel_fmax,\n",
    "                                  config.segment_length, \n",
    "                                  config.max_wav_value\n",
    "                                  )\n",
    "\n",
    "train_loader = DataLoader(train_ds,\n",
    "                        num_workers=16,\n",
    "                        shuffle=True,\n",
    "                        batch_size=config.batch_size,\n",
    "                        drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e45cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c052cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desild/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/librosa/feature/spectral.py:2148: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  mel_basis = filters.mel(sr=sr, n_fft=n_fft, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,UklGRmQfAABXQVZFZm10IBAAAAABAAEAcBcAAOAuAAACABAAZGF0YUAfAAAMP/NRXy3P/v0uxiKoDUIiewyPAhPmAt7J+yjzkvr9/kv/xvhe39PgeQD9C0r27tnp5Zfyqea12bvhEf4DCAke1uwP2D4I/fed+R0FQuub5g3UqN6N9QPrPfj27YfuSujo1j7o5wGU/M3be8pY5uLqB9YRygDZZfCK8TYCyclQv5rhCcgK1+XWh7rWvWup87jJxy/C7NUAx2LK38XUverNXt2y2+rG+8Jo5bfmRNIwzVzh1/jB/5gUzdp4znb4Le5w+JHwEd3h2A7Cidmd40TevPDQ3JveXd3g2qPps+sY3irQNs3043bb2cViyljaIOuG84sGYslEuqft6usI8mvqr9UQ1l7I79+h8MrrFfsm8F/6YPyz938HYgrYADv88v9kESEDffAR9u4KcBpxIlA7oPwa50IY5g1DERMGmueu6SfXgem/8/LlqvQC45Hpg+0y5dXuMu6H6hjqivHi/5Hpp9qn7NkHmhMLFv04sQWY6EYcCBdwGWsRk/FN+DrssvDi8frrOwlX97nxJvvz8sb43/jD8u31NPUJ9mLhGtFh3cDwaf7r/n0a9e4Gzvn7Vvnb+grz9M9O1XbMP8qux1/Aid/P17fS29XX0//Yz9BM1X7ecddP1f3GML5ryfrOcdoY3VsF/+jrqzDa8OwX4dLYj7gvyIbNbLmXu3C83Nqj1FLAIMrezNbL2cEdxOXXidD1wIG3ca28vdK/1sjazsHuRO24oba9/eSO2Jnb27vmwJHOZqxfuOnGqNhw3mTPod9q4pLdYNyv5R/0U+4i46PdEtjK4bTo5/In+akPFiRJ4arbxQ0r/NbyPuSZ16rmENLLyiPUoeFX9Bbi9+NS8ZHkRddH5tb6v/Mz4wLfstuT2mbqrvkf/3sEgx4GC4zeewRfEJj/kALb6jfvQuwZ15viCvPV/A//qO0u+awEF/nY8QcI9BMd9Yvv8PI/7FbxFgHmCFkKEBtBOw3+hOkUF4EQOBUi/OL0VwRr3RvXhfcbBQEOH/Ac/oMFa/11ANcN+RqxBnYGkwgbDSkLWhdDGQ8b9xsqVMtGn/2+Fq8rdjaTLToM7BQBEm33EwlWC6QnWxti/P8VngWx/awJzwl/Bjv28Px9BJfvWvta/mUBWgMEE01HFQf5z+UD6R1mJRUE6OEN+8nw/eAv7hT/7x8a/KXrnwTSAV8KQwWlDpMPGgXN+6f4y/6vBuUHYAhCC9gmBj6l9J3YHBQRINwFZebj26D6M+DozB/ccvN8AeThyuEG8gX6hvTy9Uf97vzL7vjtH/QY9QoDW/8sAGMK1TRaRKH6IfeDKwAliiKBDl8LaB1L9xHvvf8VHKEnawc4BV4QgBajFQ8LcBahEUYEYgnqCP0NMwyOADv61ASdOkVDa/Aa6nAZ+iTHGZfskfnvCDzjEuVx+d8dARcl8Hr0Zw4pHfb6yvWDEs0NowigCLYPpBLCCKUE2An/K2ll80Y85l4AejfDL/wjtAQCFdAVfeX+7uQPHyzDEhL2rgsxGuUR7fblAlQemxnhBvf5DQd2Cn78qPm1Dqcwsk7qIOvV7/6bKKorJxxp918MCQOL6r78HAz5JqMYeQtbJO8hVRZJGBUqRipHHBccpRGcEhUULBb5I+MaGSFbRC4udeSS9JAuxyP5DHbuUPfk97jSEtRw3xf+ePr93gfcaOMP7/fq5+AU5aLg8NbgzAHVqt6G1gPYw+Ju6gkQIhSCynDUov039oTvmdTv2qTf075vxy/YN/kT+HDJnsC81afm/dq2yYnXm9dlw02/AsLJypbEncIBwwHOMfMEATa6IKTv03vU89a/wI+587+lnYKdHr/G48vpNLoytGjUauKY2HXLfd0W3urRy8gv0H3bQd+D1BTUzuujEIAhvNgmwCjpu+um8ZTjXc+XyJKra7Tmy4vntepZvkC1OMjt1UzTCcq116nas8hhwuzIZeHK4WrXaN9c85UT8izu99LQ3PvwBZwSJQ6U+yv8h9hy0LXvZwiLGJ73fOTZ+jsBdwCq8KfvsfGj43rdadoI7FfqV9iz2V/skArOIOH3SMKK4RbuHubH67DoJerixaCwadVO8R73oN9H0Zfo7u236CbeUut0+bPtDu+16GPwEPXR6QHt9/kGHJ0weBam6Zb9MRKp/CMIIgnoBaTvL9lJ8DL+owBX9rfsrAHpA4/08uUr6LEAYwJ5+rHyhvZP+Cru6vLUAYgb/iGuJq8VFABoFWIIVRv4IeMEWAqm8rDzBQMeAt4K+wAdCCEOJgKV+mLoSfbuBED1fvcC+sL7lvCG5O7yp/3yBQQQsAs36d7zA/ab77T2ktYL5D7lTdfa5AfhC+E30hbTCecH5JXeftA325jqaeOa24rnlPSA7Yflke/xAk4JwQPVDEsAXwv3JpEQaBHs/s8Adw3t/tQKQwl+BBz9qfU2Bx4FFfbK6njvdgIh9Xzwk/ikAYQHiPMA++8I9gA88hEEkgACDd4nGQzGDr75w/hdBU/2bP5G/6X/fQVxBHIV+hLAAur96AdrIc0cQxLmGmQdjx6/E7IbOCqSHh8TKRqgGxsQwyf6DJwE7Pcy7XIAf+MM4Dzh4t9C5vXhE+i451DUJczzzsLkBOgS3bvlsORF6Dzi6OWs9WPyZ/QN9U8CPPPaFGIc8wLoCL/qPfkp/NrzHvWz7H72+fto+P7/VvOv7UDuD+sA9pDvhPWZ+3PujuY77t0CXv1s87HtLACX84f9KiTOB3EDGuaT5YL3UOXB4qzecOW664ThTe4f7wfmNd6z3O7xsPgV9tXuau6O8AHzSP5IATL4GfPN+ksJ//DoDroGguzd7iHYyfFV4ovIIsr5x53O7cMivmfI5LQss0y0m8aSz7XCGMRZx+vBe7l1uTvIM7xiry64CMoJxuy0duaa0H/KXsTVvw3Um69HsC3AEMI9yNjAIcUhyEK7Ar5quHbBmsHExUjLerzssmO0kLwOxfi13bn0teHEbrflvv3pbsfZz/G70b+WzRS1kb2FuLW88tEqyjbO3Mntwf7AdsKC2W/X2tLq01vTEdww4gfom/Hm63XuhOZI/57wcQWwESjs5wAA63D7H/WI4CbtAOj28A7zbeX97NLhattl45zuF/KW6AvtYfEl87PtY/kRB4oAvOnF5xj71Bkm9vcKkCDxDGIc1QBrEgEOgP8xCb4VTSQtH/wVMiNcH78hFSfeJF0tgSXRLFQy0TXrONE3sDYHMsQmWzWrPpRMUSi2NlhZLjfyO+khjytSKYYG3xEUG64X1RQPBoMTLA84DCYOhg07FqYK0xkVJhomGCPtHLAgsB2rFxEqPzA2NYoRJjrxUVI0ojpIHGQvlSSqDSkcrxgdG7gRcwTQGk4UrhCjA6f+5QXi/roIZQuqA9P/ZvxR+8735/TUAsr8oABE56wXFSqTBO8IePcXEnwB5Om4/cYCvgGp/179xhMcB7//gwAaB+oSAwlfGEUc9hkLFSsM6Q5OF5MVZBNECrQdUwbjIVIwFQh8GGv/Ygo3+kvf8PHY7LPv1e0Q58X3Xuhp6CbtSu7p9NvxbgDI/y76/fY9+4kN2Q+eCFYQ8hJOKTgNaSskQF0dNC7NF20ocB0BBAgSUgyvF4wWkgt2FzUMqA1ECnQNXxbEB8cMIghXBewEcAT0CowFufsxBZ4HSh2jANMN9Cv0BsAVpvojAMr+1uTD+qD6hftO++71vwKk+935MQEtBG8Jof24B/kJFQP6AKgDmgrVAQHxLvsz/xsSHvlh+HYrbwO5/drmIOYA8ePHNtAd0C/MiMhhs4bBTL0Ht0nBtLotwUS5nMPbyQ7BYcGAxb/Kssj1urTJhszS3t3QPctwBynfkdsDz9TMQ9zQsaK8S8P3w7rAwa6Dvz+8cLXJunKxY8DJuT29jr1PupS7BLoJuPit8qepsC6zW7efteig8NNnvsKuxLUio/SzzJD9kwikjZ7RoEOYxKHmqEqcQaEAoNmrFKwgqmG6qbvPtxa3GLpRv5u8KLopxpTMG9lJvrzlSPFaz/TVmrxH0uTEWrTVvsC7fMjNwE28RcRuvIvEScPKxQjQWdJP4+vZedZl2CDfSeVq5j/nePAD8KMBIvqgAcgq8wJXBU7+XgQxCMnnO/Kl+K8B0gHG+NUDCwWL/EwAhQK4E1gWhxh8ErIK8QuNEJQRMgg0+xgE4gzFFOUSMP7wLX4aeQftDhr9Cgza6DXgF/NP9ob5zOze8CP4afEN8tfy0QPXBrEDKAYpBmgRjBHuCAcEkQQOC7QWahQaIq4KQiL8ObYVPCQ4EiEeYBUp9xQHLAvdCSf77PWGBBT/Dvo0/1MDuw75ATQE/gsaEDwPzATHCKsN0ga/CbMFZxZXEtIJZDcJGpgbNxieCZ0W9PNP/1cNfAtGCzb/lwoiDKoCcw4WC7URIBLKFuoi4Rs1GLURHhnYGoYNEwvgD6YTKBlr+x8YvSEY/X4FpvB+AnDtjtfl7JjtQO5/2uneXvAg6DniDN3N5YPz8Oem773uF+6S6pfqPvMG7wDrrPJQ9AIAQPmz+voc0fUC+S/zjO+B7QvH99+N5x7mdN7L24HtN+Mu1pXgMOoP+Jfuse799dv3IPXv6671TfXQ8FbyifkKAycMJPh4HqobeAeKEvz8VQsk8fXzDgPK/9kBfPh+BGcJRfzjBlURJxxFHW0ZHikZJw8lHx0+JPooLiM9Hg0rhi7VOMYmeiruRtUh+SvuGxUbuReQA/AR8BHYE1MH+APdDF0HpgOGCcEK7hX+DpIZCxzBFI4TsBO4GxYWCBO8HkMhBCW1JwgXbUOJLKwbPyGdD1ogbwE1CJsRDhAEDCwCtw23C8H8F/8d/iIIggf5APQJQANlAtL0uvlO/JD4Hvkq/FD/3g1V92sHdxPC8PYCxutf+Nrt5uFC8tLuu/Xv6YDrLfPr6v7qDfG28Yz98/ZV/rX3FvQ99qn1R/v886PxzPic+Nv+K/jz7vQQGvIf8cnm7dc84DTDxcsEyebIcsTdubO9CbrsrQy0DrSLvmW72rmav9S6rMPTuwTD57s2tCu9ycRvyGnPFMnb2h3iXsyO1fLIYc2TxfO8+MWIxqPIR8GLwq/Gp8AJwvvD/8L9xYO8zrvwwUO8r7dGtTSvYqofpw2rW6sJroyr67RptFq4Brd8rf+tkalNryqozKw2sMWv+6/ms3W8gMGCwtvG8dCT1YLY7Nvw5bvpC+FZ5Y7nheeg6k7suPKC80n47vv/+w/23vid8InvSvUk8ozvL+Rt48PfXN+w4nzeFd784Wvp0/JZ7g71hPjJ9kbztfie+732uf38/FQEwANRCvoNYBBqElAZohtmGKUgcCNiIhUcShzqG6QZuBf4Ew8VaxiQFQQYhhOwD1oNLQnzBuMDEwGA/nn9Q/wi/A/41PhT9K7x5/M89cP2Efu1/qsCdQUWByEHNARDAST9aP5i+OD4G/uhAQMGjQSuCLQIUglTBwUJAwy6EDkUzRZNGI8a3hw7GU8WBhXXD1MQ1whlAn4Fbv4x/vP5af3WAv8DjQRCBccIzQbAAmT+kf4w+3v1Oe0s7Dnp4eee6sDo+Oic64HwK/X7+Wb9bwD/AekDGAnFDSgNzwsFCKIIogvMCY8IIgLdAV4CIwGOAjf/BAKkAP3+aATXBcEDHv/OA4cQYg8fCEb/tvet8HrloOTm5hXoiOhE6AXrk+088Hv0i/MC9tL2Afj3/VD9KPwO+p/+NwJKBMIH7AYDBsgHfgu0EJkSFBKUFhEbPhxGGxUdgRtYFncU+RPjEgwUgBTTGGAVbQ/HEG8PzA/UDYkLug0SDMUKCQxrCRcObxCJFm4a8iTMKnMpsil3JTQnBB7uHSQd3B7LIG0jYSg+K8csOCsrL5wtyzA5Llsw6zDGL+gy8DQQNjswGCzOJSgeuhYKFf4UThSfD04OgRDhEMMNAAqzCM8EsQH5/1n/o/j181DyzPNt8lrwyfGp7+ru/+wq8q36qPxA+dn2XvZt8mfrc+Yx5r/lsOPm3/7jMOSu3ZTb6tko3Ovcmtyt3BXa9tMez2TM78lkxP+/Z7/pugq1CrR8tiyzxa/yrdatQq6mp52gxaA0owqivJ5VoVqn46ekpRCouaxerPateq8Jr7uxTrFrthm16rQDvBK2lLvdtbq4oLzWtBe62bc9wUS6Xbg6ujC4YLiPrsWvNq0rrpqsPa9asOGs8KkgqZ+q3aVhqDCqCqw6qVuooatRr56xebD/s/Czi7YJuCu6273pwJrFWskAzVvQdtCa0OXQYdK30wLUn9ix2Tvb9trw3A3e+NoN2cXcRd4T3y7fGd834vba99Q41Uvby9iq0mbQItLN1OzRpdMB1hzX6NRW1fHWONaI1hPd4t+93djgu+Jp5Fri7eSa6v7t/usQ7KXxjfbq+J76wAJ5CM0HCgSEBQQFnwb5B8YH0wdvB/oHQQfgA+8BJALC/7r/pfol+077yfk0+Pz1N/WU8oXyIvV79cDzivTC9r/3RfWD9rD18fQe9qP6Ff9l/8v/2v/I/87+nf7DAowFYgI5BbQKtAzmDboPPxQUE78QlhTUFwIavBdqGDccNhw+GGwXYhjPGawcCRpVGpUecSGYHAYbNRcLFvgT7goIDVEKhgzDC8QKiRGKEEkTShQNGLQfrSFQJVgnbyxTM/A1XjekN/87Azy9Olg7HjusPJQ6ajoyQCE+WjxqPBM4XDiYNLwyXC88JzUiCBu+FrQUgw5RClcE2gGZAJP+SQBP/Vb9Pf6V/40AcPsP9x32pPNz9c30qPFk9m/5+/p6/zgIywtTCI0GHP/6+pX2cfBQ8bTu2u3i7ADvOfLG7xvxqfME9Hz0QPTv80nwJu3B7PLsbe4a7H3qn+t67KXqBekq7CPyY/N28Bbuwew+8SL3rvqm/1YEigj1C8ULHw/KETgVdBkLG+8dIh70IPAgOR2VICgm6ifzKEAqUy8fM9w3xz4qQlFEUkDvPDw4KTLlKVohRB6MHMcebR2wGO0Y5hp2GsgZvhdrGGQZ1Be5FzEZnBnoGCQYdhb9FBITbhFGD64Pbw7GDvkPqRBeFHcVyRRBE+APMAtiCYIKkQlrBY4FIwWBAnsBh/6F+7n60fjj+JP3e/IP8DjsT+m34+vfAd9J2z/XNdPn0/bUUtKr0NvPNtJX0cPMx8kpygrNq8220YzT3dSt0+XQ9dAW0njUctSJ1AzU1NKez9bMKc1TzYPPDs+7zB/LfMukz6DPFNDE0LLShdDtyp/LL8vzxd2+Fr4kwSnBDLy2tRKzUrEGslmyq7TFtr+2aLUOtKyzs7KvsLyskq8hr+OxX7T5tEq2PrXqt1O4kr4+w3bH18uyzBDNJs31z73R39T51enUiNMI0Q3TTNNi1W7XJ9er2QHXXdVD0f7SKdQ/01jVsdIr1grW/Nd42OLZjd1D3nje9tzo3oXfoeO35KbjJOi365Pts+yt8VD0dPFx9Qf5x/wl/Mv8M/7S/zT/Mvsa/Iz5ofqE+8D87AAF/9b/PgIVAOf6tfh+92z2QvUF9Ln2gfVm9PbyIfKm8JfucO1L7QrvkPCD8DHwT/J37oDqQep/7S3tfez+7aTuUPAd7+Puqu/v8V/wsOwg7UTwXPMO9h73Bvn7+7b8N/xy/Bj7xPlv+lT6rfqB9yz3ufat83Hw5e1W7fLuZ/Bm7lHtxOwv7aDs9eoD6Efn8uoL78Pv4vDN8pfz//Rv9Z/59/6vAGoEjARZAwgEqgOiA2sCiQf1CvQLQg10DgUQ6g2rDFQPUxEQEl4SLRKVElQO2wy0C0QKuwd4BZkF/AT0BQ0GAQf0Bx8INQUqA1QA7P0//b/87v17/9QA6vwU/Fj77/v//bH93ABgAe4CEwJFAdwBHgB/AG8AHQDG/Hb6Jfur/Gb+uPsE/Kj8sfu5+j79MgKw/3P+svx7+yT7V/ne98/3efrc+Jv3M/d69wP4hfUF9gn4yvoc/Ob7vP6o/vX8eP1s/nX8rPqg/uoEKwlPC6EReBXDFScU0hWnG20dUCGeJVwpfinxJ1gqxS/oMVkxLjJaNBA1yjUSOR069TdJM/IvKC4YLooqHyj7KK0pKik0J0slHSMVIeMcZx4YINEd2xtXGz0d8B0QHYwbwxqvG5gboBo8HdgbKBbTFIUVCBPgD+cPEhDMDmIMyAqYC7ANKAzRByoIsgYrAfj9Vvw5+cT0Pe8k6qTlP+KO4t7fyN2e2gTWLtT20PfM1cjEx1fGNsaZxyrGd8TVxPPGCMmdykbLX81bzoXPqNLs1GbXrtrF2h7bZN634M/jXuXy557se/Fp86Xwie/v8Wnxm+9g7nPsVuuE5zfj4OH34Hffst+D3iTdIdlv1QfWHNWu1LnScNBhzULKycrny4jMVswvzB/JxMiUzlfSw9Ke0TnT/NQE1YrUPdV42RjciNtf3WXfZd/83Qjd1Nwl2wbZFtb/1WvRFMx7yQjG8MDFu6+9C70RuU226LUXuFC4SrbEte2347YZth66yb05vfO81L+Pw6rGTsahxsjGY8jjzBXQTNKL1bHY9tuG333iHOU05fXmq+q37BHqAOpg7mzy7PLb8mz1vvQw82Xz5PNn8FLu9PFS8qDv3O157V3v4+2o7NrsdO9u8MfvOfFw8jX07/KN8nPyg/EN8b7x1fHA8nXy1vIo9Ov2hvmT9+X28vjQ+yH9mftC+5v9N/5i/mn+4f7e/H/7FPuQ+jH58vkR+Qb24vO47pvsAurv5zvmn+Vn5bThrtzV2pTaX9s/3tjgEeNY47zlQemI6u/rVe0h7zX0zvpP/moASgM9BvwITQq+CucLHQ1yDhkSjBQ8FmwVnhU5FyoXmRXLFI8VlhMgEGwNWgziCfoIAQaMBSUFowJQAcL8h/4lAZsB3gCc/xsAA//JAXMEFgWvBcQDMADjAHYDkgPfA40FlgaZCbcHbwRWBK0BXARyBusHYQcvBuMGvQbgBl8GlQeqCWYKpQrxCnIJWgekBt8H8AgGB18IjgaeBQQGVwS5BwUGvwXjBfgEbQewAgv/bf20+3X+FwAHAc0CcgoiCgkLyQ2kC64NeAtzDrMT4xckG/8gIyXmJGcqkyy8LVE2XDudOvM+XT7lPu1Bvj/IP+49+j04Ohc0ty+vJusmQiT2HvAfOhyGGgcZfRYeE4kT/hOSFM4TIRHFDxUPiw8/EhURchL3EtgSBBhZFQ0YrBbOF18ZUxhxGb0VAhOsE7cTehNUE6sOggqCCZYHawZGBncC2gKeAJf94/mc+hj3o/Ii877u2eYY4CPiP98b3BjahdpC26bY09hF3Nzcpdmu2KrbFt5M3pvb/Nkd3afcs9p23Pfc3Np/2rfaYt4A2ijZndwY3+ziT+M15Z7i+d7v3WLiAOS35lrpg+dR5eXhtt1A2YzUdtNK0zrO+c6t0prQ7sorx5nEDsd5x6nIZ8o7yX7HkMdqx1zDHsLdxYTJfMeGxXnDxMlfzHjSYdXi0/zYFNj51lvZNN093zPj1uQp5WvmwOYL5Vnm9OfS5vnmquNF4LbhC90T3srdc9iP1rbTE9H2zRvPh80rzw7Pd8+m0uvQE9Br0NvMecsuzx7TRNfK2BbclN5d3eHcxN/A4JjjGumZ6rLrG+qH6UXuXPG97zTwrPKv8m31ZfVO9f721fXD9BDzxvbM+Pb0pval+8r2QfFQ89D3P/ai8hr0i/Xc9LLvF+1Q7OzrnutR7lLvzO+X7r/rB/An7qHq8u/C9E/wyfKJ98b4mPl8+zD9w/28AAUAcwfCBmECMALWANABKQTXBw4LwQ0XDCcMTA34BzkDagRH/xL+1/36/VX8hvh6+DP1lPIv8Sr0c+9g7I7t6+nS5lPl0Of15lnmrOqK7Xzsu/Bt+S7+NwKhBxoOOhNrGgUdURzOHOQXhRJREDcOTw1EEDkRnBbTGAIXGRnYF6oXwRvVIj4qCTDQO6tFGzh4JtkiLxdTAgkCHA7IDIgJrBBZFB8O3QikCdMNShRiHjAroDTQR1BWPEWKN6My5BkZBS4PWhcqFIAeNizkJ1Ufxh5SGR0V4hsqKNgxwkAIWFNSYjWuMKQdNesG4tT7gfSw8UQQsg5R91/4M/J04Tbn7fN4+hUKdSa5H9j8N/0T8Hy2yK550pfFLcb39ZH4/uhn+k3x2ts27dP0sPShGI88iCqrD0gdtQLYwvzOXOz6y5jcRQ+W+qDrtv4S4BDL4uWw22zcbQrFIOkBdffbC3vTvZ0SxQDOSKmi2Ez+xdzw7Zv4p8mQ21Hxtdbe+zIz0ypcDJoh8heuzzjCwONyzVbNqQgi/U/0qhiE8DLWTQKQ42HYzxm4KDIZ1xbcGZv3vL5ut7LEpqaOuV/ors0g4Wb0EbTQwTzc6amo0kQEKflk/Aj3sPHgzqub1KwbtY6Sy8jR2Ay7HfFY2Nmj8d9jy4+rTPhiA90J1wkW9ZD8VcRunrfAiqNepSrq18lT2mcBnbvzwIzfdKNwwvHrBuKTCz3k9N8Z6rWVYpWSpgGAy62Nx9yrHNyl1WOtCMotw+Cq/dBh3Nv2JgNj31776thJpNiw06GNoZXIjMkf0l7wStjQyybcS81py1fuQfVbFmsRRPvcDbHZ+rumtjqkbKhcvl3FzsuI3e7ICL/KzK2/5sZ54UnmHgIE+vbri/e0zn22jrOnqRGrkcNx0LHS8uZs2QLT0+C31EvfeflV+JsZBxjEDd8cSP2T65TbGNY81lvr4/fD+g==\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.0326) tensor(-11.5129) torch.Size([4000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "for d in train_ds:\n",
    "    mel, audio, orig_audio, len_audio = d\n",
    "    plt.imshow(mel.numpy(), aspect='auto', origin='lower')\n",
    "    display(Audio(orig_audio.numpy(), rate=6000))\n",
    "    display(Audio(audio.numpy(), rate=6000))\n",
    "    print(mel.max(), mel.min(), audio.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "210eb6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Diversity Loss (L_div): 5.6051\n",
      "Goal in Optimization: MAXIMIZE this value (e.g., minimize -L_div).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiversityLoss(nn.Module):\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Initializes the Diversity Loss module.\n",
    "        \n",
    "        Args:\n",
    "            epsilon: Small value added to the denominator to prevent division by zero.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon \n",
    "\n",
    "    def forward(self, features: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the feature space diversity loss (L_div) for a batch.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): The generated feature vectors (F(G(Z))). Shape (B, D).\n",
    "            z (torch.Tensor): The latent input vectors (Z). Shape (B, L).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The mean ratio (L_div), which should be maximized.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- 1. Calculate Pairwise Distances ---\n",
    "        \n",
    "        # Numerator: Feature Distance Matrix (BxB)\n",
    "        # ||F(G(z1)) - F(G(z2))||\n",
    "        feature_dist_matrix = torch.cdist(features, features)\n",
    "        \n",
    "        # Denominator: Latent Distance Matrix (BxB)\n",
    "        # ||z1 - z2||\n",
    "        latent_dist_matrix = torch.cdist(z, z)\n",
    "\n",
    "        # --- 2. Compute the Ratio and Exclude Diagonal ---\n",
    "        \n",
    "        # Compute the ratio, adding epsilon to stabilize near-zero distances\n",
    "        ratio_matrix = feature_dist_matrix / (latent_dist_matrix + self.epsilon)\n",
    "\n",
    "        # The diagonal contains meaningless values (0/epsilon). We must exclude them.\n",
    "        \n",
    "        # Create a mask to exclude the diagonal elements (i=j)\n",
    "        mask = torch.ones_like(ratio_matrix, dtype=torch.bool)\n",
    "        mask.fill_diagonal_(False)\n",
    "        \n",
    "        # Apply the mask and calculate the mean of all non-diagonal ratios\n",
    "        diversity_loss = ratio_matrix[mask].mean()\n",
    "\n",
    "        return diversity_loss\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Setup\n",
    "BATCH_SIZE = 32\n",
    "LATENT_DIM = 10 \n",
    "FEATURE_DIM = 256\n",
    "\n",
    "# Instantiate the loss module\n",
    "l_div = DiversityLoss()\n",
    "\n",
    "# 2. Simulate Input Data (Tensors must be on the same device)\n",
    "Z_tensor = torch.randn(BATCH_SIZE, LATENT_DIM) # Input Z\n",
    "Feature_tensor = torch.randn(BATCH_SIZE, FEATURE_DIM) # Output F(G(Z))\n",
    "\n",
    "# 3. Calculate Loss\n",
    "loss_value = l_div(Feature_tensor, Z_tensor)\n",
    "\n",
    "print(f\"Calculated Diversity Loss (L_div): {loss_value.item():.4f}\")\n",
    "print(f\"Goal in Optimization: MAXIMIZE this value (e.g., minimize -L_div).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38d4cc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SincNetCompound(\n",
       "  (cnn): SincNet(\n",
       "    (conv): ModuleList(\n",
       "      (0): SincConv_fast()\n",
       "      (1): Conv1d(100, 80, kernel_size=(5,), stride=(1,))\n",
       "      (2): Conv1d(80, 80, kernel_size=(5,), stride=(1,))\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(100, eps=1250, momentum=0.05, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(80, eps=415, momentum=0.05, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(80, eps=137, momentum=0.05, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (ln): ModuleList(\n",
       "      (0-2): 3 x LayerNorm()\n",
       "    )\n",
       "    (act): ModuleList(\n",
       "      (0-2): 3 x LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (drop): ModuleList(\n",
       "      (0-2): 3 x Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln0): LayerNorm()\n",
       "  )\n",
       "  (dnn1): MLP(\n",
       "    (wx): ModuleList(\n",
       "      (0): Linear(in_features=10960, out_features=2048, bias=True)\n",
       "      (1-2): 2 x Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0-2): 3 x BatchNorm1d(2048, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (ln): ModuleList(\n",
       "      (0-2): 3 x LayerNorm()\n",
       "    )\n",
       "    (act): ModuleList(\n",
       "      (0-2): 3 x LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (drop): ModuleList(\n",
       "      (0-2): 3 x Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln0): LayerNorm()\n",
       "  )\n",
       "  (dnn2): MLP(\n",
       "    (wx): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=35, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(35, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (ln): ModuleList(\n",
       "      (0): LayerNorm()\n",
       "    )\n",
       "    (act): ModuleList(\n",
       "      (0): LogSoftmax(dim=1)\n",
       "    )\n",
       "    (drop): ModuleList(\n",
       "      (0): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SincNetCompound(nn.Module):\n",
    "    def __init__(self, cnn, dnn1, dnn2):\n",
    "        super(SincNetCompound, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.dnn1 = dnn1\n",
    "        self.dnn2 = dnn2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.dnn1(x)\n",
    "        x = self.dnn2(x)\n",
    "        return x\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.dnn1(x)\n",
    "        return x\n",
    "    \n",
    "aud_disc = SincNetCompound(CNN_net, DNN1_net, DNN2_net)\n",
    "aud_disc.cuda()\n",
    "\n",
    "sr_model = SincNetCompound(CNN_net_sr, DNN1_net_sr, DNN2_net_sr)\n",
    "sr_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1837e3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud_disc(torch.randn(2, wlen).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "80777f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (l1): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "  )\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (12): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (15): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator.apply(weights_init_normal)\n",
    "generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4507af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
    "\n",
    "LRELU_SLOPE = 0.1\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(mean, std)\n",
    "\n",
    "\n",
    "def apply_weight_norm(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        weight_norm(m)\n",
    "\n",
    "\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "class ResBlock1(torch.nn.Module):\n",
    "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.h = h\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                               padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, LRELU_SLOPE)\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs1:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.convs2:\n",
    "            remove_weight_norm(l)\n",
    "\n",
    "\n",
    "class ResBlock2(torch.nn.Module):\n",
    "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
    "        super(ResBlock2, self).__init__()\n",
    "        self.h = h\n",
    "        self.convs = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1])))\n",
    "        ])\n",
    "        self.convs.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c in self.convs:\n",
    "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            xt = c(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs:\n",
    "            remove_weight_norm(l)\n",
    "\n",
    "\n",
    "class HIFIGenerator(torch.nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super(HIFIGenerator, self).__init__()\n",
    "        self.h = h\n",
    "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(h.upsample_rates)\n",
    "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
    "        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = h.upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(h, ch, k, d))\n",
    "\n",
    "        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
    "        self.ups.apply(init_weights)\n",
    "        self.conv_post.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "hifigan_config = {\n",
    "    \"resblock\": \"1\",\n",
    "    \"num_gpus\": 0,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"adam_b1\": 0.8,\n",
    "    \"adam_b2\": 0.99,\n",
    "    \"lr_decay\": 0.999,\n",
    "    \"seed\": 1234,\n",
    "\n",
    "    \"upsample_rates\": [8,8,2,2],\n",
    "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
    "    \"upsample_initial_channel\": 512,\n",
    "    \"resblock_kernel_sizes\": [3,7,11],\n",
    "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
    "\n",
    "    \"segment_size\": 8192,\n",
    "    \"num_mels\": 80,\n",
    "    \"num_freq\": 1025,\n",
    "    \"n_fft\": 1024,\n",
    "    \"hop_size\": 256,\n",
    "    \"win_size\": 1024,\n",
    "\n",
    "    \"sampling_rate\": 22050,\n",
    "\n",
    "    \"fmin\": 0,\n",
    "    \"fmax\": 8000,\n",
    "    \"fmax_for_loss\": None,\n",
    "    \"num_workers\": 4,\n",
    "\n",
    "    \"dist_config\": {\n",
    "        \"dist_backend\": \"nccl\",\n",
    "        \"dist_url\": \"tcp://localhost:54321\",\n",
    "        \"world_size\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "hifigan_config = dycomutils.config.ConfigDict(hifigan_config)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6df8bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desild/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HIFIGenerator(\n",
       "  (conv_pre): Conv1d(80, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (ups): ModuleList(\n",
       "    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "  )\n",
       "  (resblocks): ModuleList(\n",
       "    (0): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (3): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (4): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (5): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (6): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (7): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (8): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (9): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (10): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (11): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hifigan_weight = torch.load('/home/desild/work/academic/sem3/TrustworthyML-assignment/tacotron2/vctk/models/pretrain_hifigan/generator_v1')\n",
    "hifigan = HIFIGenerator(hifigan_config)\n",
    "hifigan.load_state_dict(hifigan_weight['generator'])\n",
    "hifigan.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "952883da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2862],\n",
       "        [0.1554]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_disc = ImgDiscriminator()\n",
    "img_disc.cuda()\n",
    "img_disc(torch.randn(2,1,80,16).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e9123b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 16])\n"
     ]
    }
   ],
   "source": [
    "mel = generator(torch.randn(1, opt.latent_dim).cuda())\n",
    "print(mel.shape)\n",
    "audios = hifigan(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e30afb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4096])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fb6eb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "optimizer_dis_aud = torch.optim.AdamW(aud_disc.parameters(), lr=float(lr), eps=1e-8, weight_decay=1e-4) \n",
    "optimizer_dis_img = torch.optim.AdamW(img_disc.parameters(), lr=float(lr), eps=1e-8, weight_decay=1e-4) \n",
    "optimizer_gen = torch.optim.AdamW(chain(generator.parameters(), hifigan.parameters()), lr=float(lr), eps=1e-8, weight_decay=1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cbcbe9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/500 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     29\u001b[39m img_validity = img_disc(mel.unsqueeze(\u001b[32m1\u001b[39m))\n\u001b[32m     30\u001b[39m audio_validity = aud_disc(audios.squeeze(\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m g_loss = -torch.mean(audio_validity) + -torch.mean(img_validity) - \u001b[43ml_div\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m g_loss.backward()\n\u001b[32m     34\u001b[39m optimizer_gen.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mDiversityLoss.forward\u001b[39m\u001b[34m(self, features, z)\u001b[39m\n\u001b[32m     46\u001b[39m mask.fill_diagonal_(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     48\u001b[39m \u001b[38;5;66;03m# Apply the mask and calculate the mean of all non-diagonal ratios\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m diversity_loss = \u001b[43mratio_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m diversity_loss\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "with tqdm(total=config.epochs, desc=\"Processing Batches\") as pbar:\n",
    "    for epoch in range(config.epochs):\n",
    "    # Used to calculate avg items/sec over epoch\n",
    "        epoch_logs = []\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            #(mel, aud), y, _, _, sp_id, _ = batch_to_gpu(batch)\n",
    "            true_mel, true_aud, orig_audio, _ = batch\n",
    "            true_mel = to_gpu(true_mel).float()\n",
    "            true_aud = to_gpu(true_aud).float()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_gen.zero_grad()\n",
    "            # Sample noise as generator input\n",
    "            z = Variable(torch.randn(1, opt.latent_dim).cuda())\n",
    "            \n",
    "            mel = generator(torch.randn(true_aud.size(0), opt.latent_dim).cuda())\n",
    "            audios = hifigan(mel)\n",
    "            audios = audios[:,:, audios.shape[-1]//2 - true_aud.shape[-1]//2 : audios.shape[-1]//2 + true_aud.shape[-1]//2]\n",
    "            feat = sr_model.extract_features(audios.squeeze(1))\n",
    "            \n",
    "            #print(audios.shape, true_aud.shape)\n",
    "            # Calculate generator loss\n",
    "            img_validity = img_disc(mel.unsqueeze(1))\n",
    "            audio_validity = aud_disc(audios.squeeze(1))\n",
    "            g_loss = -torch.mean(audio_validity) + -torch.mean(img_validity) - l_div(feat, z)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Audio Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_dis_aud.zero_grad()\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            d_real = aud_disc(true_aud)\n",
    "            d_fake = aud_disc(audios.squeeze(1).detach())\n",
    "\n",
    "            gradient_penalty = compute_gradient_penalty(aud_disc, true_aud.data, audios.squeeze(1).data)\n",
    "            \n",
    "            # Adversarial loss\n",
    "            d_loss_aud = -torch.mean(d_real) + torch.mean(d_fake) + opt.lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss_aud.backward()\n",
    "            optimizer_dis_aud.step()\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Image Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_dis_img.zero_grad()\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            d_real = img_disc(true_mel.unsqueeze(1))\n",
    "            d_fake = img_disc(mel.unsqueeze(1).detach())\n",
    "\n",
    "            #print(true_mel.data.shape, mel.data.shape)\n",
    "            gradient_penalty = compute_gradient_penalty(img_disc, true_mel.data, mel.data)\n",
    "            \n",
    "            # Adversarial loss\n",
    "            d_loss_img = -torch.mean(d_real) + torch.mean(d_fake) + opt.lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss_img.backward()\n",
    "            optimizer_dis_img.step()\n",
    "            \n",
    "            epoch_logs.append({\n",
    "            \"d_mel_loss\": d_loss_img.item(),\n",
    "            \"d_aud_loss\": d_loss_aud.item(),\n",
    "            \"g_loss\": g_loss.item(),\n",
    "            })\n",
    "            \n",
    "            batches_done = epoch * len(train_ds) + i\n",
    "            if batches_done % opt.sample_interval == 0:\n",
    "                os.makedirs(\"images/HIFIGAN_ADV/\", exist_ok=True)\n",
    "                save_image(mel.unsqueeze(1)[:25], \"images/HIFIGAN_ADV/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "            \n",
    "            # print(\n",
    "            #     \"[Epoch %d/%d] [Batch %d/%d] [loss: %f] [error: %f]\"\n",
    "            #     % (epoch, config.epochs, i, len(train_loader), loss.item(), err.item())\n",
    "            # )\n",
    "            \n",
    "        \n",
    "        epoch_df = pd.DataFrame(epoch_logs)\n",
    "        run[\"train/epoch/d_mel_loss\"].append(epoch_df[\"d_mel_loss\"].mean())\n",
    "        run[\"train/epoch/d_aud_loss\"].append(epoch_df[\"d_aud_loss\"].mean())\n",
    "        run[\"train/epoch/g_loss\"].append(epoch_df[\"g_loss\"].mean())\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "# %%\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_type = \"HIFIGAN_ADV\"\n",
    "os.makedirs(\"models/{}/{}\".format(model_type, timestamp), exist_ok=True)\n",
    "\n",
    "config_dict = {\n",
    "    \"config\": config.__dict__,\n",
    "    \"model_config\": {\n",
    "        \"CNN_arch\": CNN_arch,\n",
    "        \"DNN1_arch\": DNN1_arch,\n",
    "        \"DNN2_arch\": DNN2_arch,\n",
    "        \"fs\": fs,\n",
    "        \"n_mel_channels\": config.n_mel_channels\n",
    "    }\n",
    "}\n",
    "\n",
    "save_dict = {\n",
    "    'dis_aud': aud_disc.state_dict(),\n",
    "    'dis_img': img_disc.state_dict(),\n",
    "    'gen': generator.state_dict(),\n",
    "    'hifigan': hifigan.state_dict(),\n",
    "    'optimizer_dis_aud': optimizer_dis_aud.state_dict(),\n",
    "    'optimizer_dis_img': optimizer_dis_img.state_dict(),\n",
    "    'optimizer_gen': optimizer_gen.state_dict(),\n",
    "    'epoch': config.epochs,\n",
    "    'batch_size': config.batch_size,\n",
    "    'learning_rate': float(lr),\n",
    "    'config': config_dict\n",
    "    }\n",
    "\n",
    "\n",
    "torch.save(save_dict, \"models/{}/{}/checkpoint.pth\".format(model_type, timestamp))\n",
    "# torch.save(model_waveglow.state_dict(), \"models/SINCNET/{}/waveglow.pth\".format(timestamp))\n",
    "# torch.save(audio_discriminator.state_dict(), \"models/SINCNET/{}/audio_discriminator.pth\".format(timestamp))\n",
    "\n",
    "run[\"model/saved_model/checkpoint\"].upload(\"models/{}/{}/checkpoint.pth\".format(model_type, timestamp))\n",
    "# run[\"model/saved_model/waveglow\"].upload(\"models/SINCNET/{}/waveglow.pth\".format(timestamp))\n",
    "# run[\"model/saved_model/audio_dis\"].upload(\"models/SINCNET/{}/audio_discriminator.pth\".format(timestamp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e46d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
