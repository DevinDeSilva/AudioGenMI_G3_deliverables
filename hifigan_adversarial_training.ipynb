{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e34b3099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "from torchvision.models import resnet18\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "import itertools\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "import torch.autograd as autograd\n",
    "import dycomutils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from dotenv import load_dotenv\n",
    "import neptune\n",
    "import pandas as pd # Note: pandas is not needed for this class\n",
    "import sys\n",
    "sys.path.append(\"/home/desild/work/academic/sem3/TrustworthyML-assignment/tacotron2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "254a15e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs-legacy.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/Botz/Audio-MI/e/AUD1-515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'tuple'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs-legacy.neptune.ai/help/value_of_unsupported_type\n",
      "[neptune] [warning] NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'NoneType'>).\n",
      "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
      "        for dictionaries or collections that contain unsupported values.\n",
      "        For more, see https://docs-legacy.neptune.ai/help/value_of_unsupported_type\n"
     ]
    }
   ],
   "source": [
    "MAIN_DIR = \"/mnt/i/My Drive/TrustworthyML-assignment\"\n",
    "\n",
    "# You can just define the dictionary directly\n",
    "opt = {\n",
    "    \"n_epochs\": 4000,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr\": 0.0002,\n",
    "    \"b1\": 0.5,\n",
    "    \"b2\": 0.999,\n",
    "    \"n_cpu\": 8,\n",
    "    \"latent_dim\": 128,\n",
    "    \"img_size\": (1,80,16),  \n",
    "    \"sample_interval\": 400,\n",
    "    \"gamma\": 0.75,\n",
    "    \"lambda_k\": 0.001,\n",
    "    \"lambda_gp\": 10,\n",
    "    \"load_gen\": None, #os.path.join(MAIN_DIR, \"Code\", \"saved_models\", \"generator.pth\"),\n",
    "    \"load_dis\": None, #os.path.join(MAIN_DIR, \"Code\", \"saved_models\", \"discriminator.pth\"),\n",
    "}\n",
    "\n",
    "load_dotenv(os.path.join(MAIN_DIR, \"Code\", \".ENV\"))\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=\"Botz/Audio-MI\",\n",
    "    name=\"hifigan-training\",\n",
    "    api_token=os.getenv(\"NEPTUNE_API_TOKEN\")\n",
    ")\n",
    "run[\"parameters\"] = opt\n",
    "\n",
    "opt = dycomutils.config.ConfigDict(opt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e95bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "# Generator Code\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.init_size = (80//16, 16 // 16)\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 64 * self.init_size[0] * self.init_size[1]))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(64, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 1, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise):\n",
    "        out = self.l1(noise)\n",
    "        out = out.view(out.shape[0], 64, self.init_size[0], self.init_size[1])\n",
    "        img = self.conv_blocks(out)\n",
    "        return img.squeeze(1)\n",
    "    \n",
    "class ImgDiscriminator(nn.Module):\n",
    "    def __init__(self, img_size=opt.img_size):\n",
    "        super(ImgDiscriminator, self).__init__()\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(int(np.prod(img_size)), 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        img_flat = img.reshape(img.shape[0], -1)\n",
    "        validity = self.model(img_flat)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43fe5973",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# %%\n",
    "def flip(x, dim):\n",
    "    xsize = x.size()\n",
    "    dim = x.dim() + dim if dim < 0 else dim\n",
    "    x = x.contiguous()\n",
    "    x = x.view(-1, *xsize[dim:])\n",
    "    x = x.view(x.size(0), x.size(1), -1)[:, getattr(torch.arange(x.size(1)-1, \n",
    "                      -1, -1), ('cpu','cuda')[x.is_cuda])().long(), :]\n",
    "    return x.view(xsize)\n",
    "\n",
    "\n",
    "def sinc(band,t_right):\n",
    "    y_right= torch.sin(2*math.pi*band*t_right)/(2*math.pi*band*t_right)\n",
    "    y_left= flip(y_right,0)\n",
    "\n",
    "    y=torch.cat([y_left,Variable(torch.ones(1)).cuda(),y_right])\n",
    "\n",
    "    return y\n",
    "    \n",
    "\n",
    "class SincConv_fast(nn.Module):\n",
    "    \"\"\"Sinc-based convolution\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_channels : `int`\n",
    "        Number of input channels. Must be 1.\n",
    "    out_channels : `int`\n",
    "        Number of filters.\n",
    "    kernel_size : `int`\n",
    "        Filter length.\n",
    "    sample_rate : `int`, optional\n",
    "        Sample rate. Defaults to 16000.\n",
    "    Usage\n",
    "    -----\n",
    "    See `torch.nn.Conv1d`\n",
    "    Reference\n",
    "    ---------\n",
    "    Mirco Ravanelli, Yoshua Bengio,\n",
    "    \"Speaker Recognition from raw waveform with SincNet\".\n",
    "    https://arxiv.org/abs/1808.00158\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def to_mel(hz):\n",
    "        return 2595 * np.log10(1 + hz / 700)\n",
    "\n",
    "    @staticmethod\n",
    "    def to_hz(mel):\n",
    "        return 700 * (10 ** (mel / 2595) - 1)\n",
    "\n",
    "    def __init__(self, out_channels, kernel_size, sample_rate=16000, in_channels=1,\n",
    "                 stride=1, padding=0, dilation=1, bias=False, groups=1, min_low_hz=50, min_band_hz=50):\n",
    "\n",
    "        super(SincConv_fast,self).__init__()\n",
    "\n",
    "        if in_channels != 1:\n",
    "            #msg = (f'SincConv only support one input channel '\n",
    "            #       f'(here, in_channels = {in_channels:d}).')\n",
    "            msg = \"SincConv only support one input channel (here, in_channels = {%i})\" % (in_channels)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        # Forcing the filters to be odd (i.e, perfectly symmetrics)\n",
    "        if kernel_size%2==0:\n",
    "            self.kernel_size=self.kernel_size+1\n",
    "            \n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.dilation = dilation\n",
    "\n",
    "        if bias:\n",
    "            raise ValueError('SincConv does not support bias.')\n",
    "        if groups > 1:\n",
    "            raise ValueError('SincConv does not support groups.')\n",
    "\n",
    "        self.sample_rate = sample_rate\n",
    "        self.min_low_hz = min_low_hz\n",
    "        self.min_band_hz = min_band_hz\n",
    "\n",
    "        # initialize filterbanks such that they are equally spaced in Mel scale\n",
    "        low_hz = 30\n",
    "        high_hz = self.sample_rate / 2 - (self.min_low_hz + self.min_band_hz)\n",
    "\n",
    "        mel = np.linspace(self.to_mel(low_hz),\n",
    "                          self.to_mel(high_hz),\n",
    "                          self.out_channels + 1)\n",
    "        hz = self.to_hz(mel)\n",
    "        \n",
    "\n",
    "        # filter lower frequency (out_channels, 1)\n",
    "        self.low_hz_ = nn.Parameter(torch.Tensor(hz[:-1]).view(-1, 1))\n",
    "\n",
    "        # filter frequency band (out_channels, 1)\n",
    "        self.band_hz_ = nn.Parameter(torch.Tensor(np.diff(hz)).view(-1, 1))\n",
    "\n",
    "        # Hamming window\n",
    "        #self.window_ = torch.hamming_window(self.kernel_size)\n",
    "        n_lin=torch.linspace(0, (self.kernel_size/2)-1, steps=int((self.kernel_size/2))) # computing only half of the window\n",
    "        self.window_=0.54-0.46*torch.cos(2*math.pi*n_lin/self.kernel_size);\n",
    "\n",
    "\n",
    "        # (1, kernel_size/2)\n",
    "        n = (self.kernel_size - 1) / 2.0\n",
    "        self.n_ = 2*math.pi*torch.arange(-n, 0).view(1, -1) / self.sample_rate # Due to symmetry, I only need half of the time axes\n",
    "\n",
    " \n",
    "\n",
    "\n",
    "    def forward(self, waveforms):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        waveforms : `torch.Tensor` (batch_size, 1, n_samples)\n",
    "            Batch of waveforms.\n",
    "        Returns\n",
    "        -------\n",
    "        features : `torch.Tensor` (batch_size, out_channels, n_samples_out)\n",
    "            Batch of sinc filters activations.\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_ = self.n_.to(waveforms.device)\n",
    "\n",
    "        self.window_ = self.window_.to(waveforms.device)\n",
    "\n",
    "        low = self.min_low_hz  + torch.abs(self.low_hz_)\n",
    "        \n",
    "        high = torch.clamp(low + self.min_band_hz + torch.abs(self.band_hz_),self.min_low_hz,self.sample_rate/2)\n",
    "        band=(high-low)[:,0]\n",
    "        \n",
    "        f_times_t_low = torch.matmul(low, self.n_)\n",
    "        f_times_t_high = torch.matmul(high, self.n_)\n",
    "\n",
    "        band_pass_left=((torch.sin(f_times_t_high)-torch.sin(f_times_t_low))/(self.n_/2))*self.window_ # Equivalent of Eq.4 of the reference paper (SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET). I just have expanded the sinc and simplified the terms. This way I avoid several useless computations. \n",
    "        band_pass_center = 2*band.view(-1,1)\n",
    "        band_pass_right= torch.flip(band_pass_left,dims=[1])\n",
    "        \n",
    "        \n",
    "        band_pass=torch.cat([band_pass_left,band_pass_center,band_pass_right],dim=1)\n",
    "\n",
    "        \n",
    "        band_pass = band_pass / (2*band[:,None])\n",
    "        \n",
    "\n",
    "        self.filters = (band_pass).view(\n",
    "            self.out_channels, 1, self.kernel_size)\n",
    "\n",
    "        return F.conv1d(waveforms, self.filters, stride=self.stride,\n",
    "                        padding=self.padding, dilation=self.dilation,\n",
    "                         bias=None, groups=1) \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "class sinc_conv(nn.Module):\n",
    "\n",
    "    def __init__(self, N_filt,Filt_dim,fs):\n",
    "        super(sinc_conv,self).__init__()\n",
    "\n",
    "        # Mel Initialization of the filterbanks\n",
    "        low_freq_mel = 80\n",
    "        high_freq_mel = (2595 * np.log10(1 + (fs / 2) / 700))  # Convert Hz to Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, N_filt)  # Equally spaced in Mel scale\n",
    "        f_cos = (700 * (10**(mel_points / 2595) - 1)) # Convert Mel to Hz\n",
    "        b1=np.roll(f_cos,1)\n",
    "        b2=np.roll(f_cos,-1)\n",
    "        b1[0]=30\n",
    "        b2[-1]=(fs/2)-100\n",
    "                \n",
    "        self.freq_scale=fs*1.0\n",
    "        self.filt_b1 = nn.Parameter(torch.from_numpy(b1/self.freq_scale))\n",
    "        self.filt_band = nn.Parameter(torch.from_numpy((b2-b1)/self.freq_scale))\n",
    "\n",
    "        \n",
    "        self.N_filt=N_filt\n",
    "        self.Filt_dim=Filt_dim\n",
    "        self.fs=fs\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        filters=Variable(torch.zeros((self.N_filt,self.Filt_dim))).cuda()\n",
    "        N=self.Filt_dim\n",
    "        t_right=Variable(torch.linspace(1, (N-1)/2, steps=int((N-1)/2))/self.fs).cuda()\n",
    "        \n",
    "        \n",
    "        min_freq=50.0;\n",
    "        min_band=50.0;\n",
    "        \n",
    "        filt_beg_freq=torch.abs(self.filt_b1)+min_freq/self.freq_scale\n",
    "        filt_end_freq=filt_beg_freq+(torch.abs(self.filt_band)+min_band/self.freq_scale)\n",
    "       \n",
    "        n=torch.linspace(0, N, steps=N)\n",
    "\n",
    "        # Filter window (hamming)\n",
    "        window=0.54-0.46*torch.cos(2*math.pi*n/N);\n",
    "        window=Variable(window.float().cuda())\n",
    "\n",
    "        \n",
    "        for i in range(self.N_filt):\n",
    "                        \n",
    "            low_pass1 = 2*filt_beg_freq[i].float()*sinc(filt_beg_freq[i].float()*self.freq_scale,t_right)\n",
    "            low_pass2 = 2*filt_end_freq[i].float()*sinc(filt_end_freq[i].float()*self.freq_scale,t_right)\n",
    "            band_pass=(low_pass2-low_pass1)\n",
    "\n",
    "            band_pass=band_pass/torch.max(band_pass)\n",
    "\n",
    "            filters[i,:]=band_pass.cuda()*window\n",
    "\n",
    "        out=F.conv1d(x, filters.view(self.N_filt,1,self.Filt_dim))\n",
    "    \n",
    "        return out\n",
    "    \n",
    "\n",
    "def act_fun(act_type):\n",
    "\n",
    " if act_type==\"relu\":\n",
    "    return nn.ReLU()\n",
    "            \n",
    " if act_type==\"tanh\":\n",
    "    return nn.Tanh()\n",
    "            \n",
    " if act_type==\"sigmoid\":\n",
    "    return nn.Sigmoid()\n",
    "           \n",
    " if act_type==\"leaky_relu\":\n",
    "    return nn.LeakyReLU(0.2)\n",
    "            \n",
    " if act_type==\"elu\":\n",
    "    return nn.ELU()\n",
    "                     \n",
    " if act_type==\"softmax\":\n",
    "    return nn.LogSoftmax(dim=1)\n",
    "        \n",
    " if act_type==\"linear\":\n",
    "    return nn.LeakyReLU(1) # initializzed like this, but not used in forward!\n",
    "            \n",
    "            \n",
    "class LayerNorm(nn.Module):\n",
    "\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm,self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(features))\n",
    "        self.beta = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, options):\n",
    "        super(MLP, self).__init__()\n",
    "        \n",
    "        self.input_dim=int(options['input_dim'])\n",
    "        self.fc_lay=options['fc_lay']\n",
    "        self.fc_drop=options['fc_drop']\n",
    "        self.fc_use_batchnorm=options['fc_use_batchnorm']\n",
    "        self.fc_use_laynorm=options['fc_use_laynorm']\n",
    "        self.fc_use_laynorm_inp=options['fc_use_laynorm_inp']\n",
    "        self.fc_use_batchnorm_inp=options['fc_use_batchnorm_inp']\n",
    "        self.fc_act=options['fc_act']\n",
    "        \n",
    "       \n",
    "        self.wx  = nn.ModuleList([])\n",
    "        self.bn  = nn.ModuleList([])\n",
    "        self.ln  = nn.ModuleList([])\n",
    "        self.act = nn.ModuleList([])\n",
    "        self.drop = nn.ModuleList([])\n",
    "       \n",
    "\n",
    "       \n",
    "        # input layer normalization\n",
    "        if self.fc_use_laynorm_inp:\n",
    "           self.ln0=LayerNorm(self.input_dim)\n",
    "          \n",
    "        # input batch normalization    \n",
    "        if self.fc_use_batchnorm_inp:\n",
    "           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n",
    "           \n",
    "           \n",
    "        self.N_fc_lay=len(self.fc_lay)\n",
    "             \n",
    "        current_input=self.input_dim\n",
    "        \n",
    "        # Initialization of hidden layers\n",
    "        \n",
    "        for i in range(self.N_fc_lay):\n",
    "            \n",
    "         # dropout\n",
    "         self.drop.append(nn.Dropout(p=self.fc_drop[i]))\n",
    "         \n",
    "         # activation\n",
    "         self.act.append(act_fun(self.fc_act[i]))\n",
    "         \n",
    "         \n",
    "         add_bias=True\n",
    "         \n",
    "         # layer norm initialization\n",
    "         self.ln.append(LayerNorm(self.fc_lay[i]))\n",
    "         self.bn.append(nn.BatchNorm1d(self.fc_lay[i],momentum=0.05))\n",
    "         \n",
    "         if self.fc_use_laynorm[i] or self.fc_use_batchnorm[i]:\n",
    "             add_bias=False\n",
    "         \n",
    "              \n",
    "         # Linear operations\n",
    "         self.wx.append(nn.Linear(current_input, self.fc_lay[i],bias=add_bias))\n",
    "         \n",
    "         # weight initialization\n",
    "         self.wx[i].weight = torch.nn.Parameter(torch.Tensor(self.fc_lay[i],current_input).uniform_(-np.sqrt(0.01/(current_input+self.fc_lay[i])),np.sqrt(0.01/(current_input+self.fc_lay[i]))))\n",
    "         self.wx[i].bias = torch.nn.Parameter(torch.zeros(self.fc_lay[i]))\n",
    "         \n",
    "         current_input=self.fc_lay[i]\n",
    "         \n",
    "         \n",
    "    def forward(self, x):\n",
    "        \n",
    "      # Applying Layer/Batch Norm\n",
    "      if bool(self.fc_use_laynorm_inp):\n",
    "        x=self.ln0((x))\n",
    "        \n",
    "      if bool(self.fc_use_batchnorm_inp):\n",
    "        x=self.bn0((x))\n",
    "        \n",
    "      for i in range(self.N_fc_lay):\n",
    "\n",
    "        if self.fc_act[i]!='linear':\n",
    "            \n",
    "          if self.fc_use_laynorm[i]:\n",
    "           x = self.drop[i](self.act[i](self.ln[i](self.wx[i](x))))\n",
    "          \n",
    "          if self.fc_use_batchnorm[i]:\n",
    "           x = self.drop[i](self.act[i](self.bn[i](self.wx[i](x))))\n",
    "          \n",
    "          if self.fc_use_batchnorm[i]==False and self.fc_use_laynorm[i]==False:\n",
    "           x = self.drop[i](self.act[i](self.wx[i](x)))\n",
    "           \n",
    "        else:\n",
    "          if self.fc_use_laynorm[i]:\n",
    "           x = self.drop[i](self.ln[i](self.wx[i](x)))\n",
    "          \n",
    "          if self.fc_use_batchnorm[i]:\n",
    "           x = self.drop[i](self.bn[i](self.wx[i](x)))\n",
    "          \n",
    "          if self.fc_use_batchnorm[i]==False and self.fc_use_laynorm[i]==False:\n",
    "           x = self.drop[i](self.wx[i](x)) \n",
    "          \n",
    "      return x\n",
    "\n",
    "\n",
    "\n",
    "class SincNet(nn.Module):\n",
    "    \n",
    "    def __init__(self,options):\n",
    "       super(SincNet,self).__init__()\n",
    "    \n",
    "       self.cnn_N_filt=options['cnn_N_filt']\n",
    "       self.cnn_len_filt=options['cnn_len_filt']\n",
    "       self.cnn_max_pool_len=options['cnn_max_pool_len']\n",
    "       \n",
    "       \n",
    "       self.cnn_act=options['cnn_act']\n",
    "       self.cnn_drop=options['cnn_drop']\n",
    "       \n",
    "       self.cnn_use_laynorm=options['cnn_use_laynorm']\n",
    "       self.cnn_use_batchnorm=options['cnn_use_batchnorm']\n",
    "       self.cnn_use_laynorm_inp=options['cnn_use_laynorm_inp']\n",
    "       self.cnn_use_batchnorm_inp=options['cnn_use_batchnorm_inp']\n",
    "       \n",
    "       self.input_dim=int(options['input_dim'])\n",
    "       \n",
    "       self.fs=options['fs']\n",
    "       \n",
    "       self.N_cnn_lay=len(options['cnn_N_filt'])\n",
    "       self.conv  = nn.ModuleList([])\n",
    "       self.bn  = nn.ModuleList([])\n",
    "       self.ln  = nn.ModuleList([])\n",
    "       self.act = nn.ModuleList([])\n",
    "       self.drop = nn.ModuleList([])\n",
    "       \n",
    "             \n",
    "       if self.cnn_use_laynorm_inp:\n",
    "           self.ln0=LayerNorm(self.input_dim)\n",
    "           \n",
    "       if self.cnn_use_batchnorm_inp:\n",
    "           self.bn0=nn.BatchNorm1d([self.input_dim],momentum=0.05)\n",
    "           \n",
    "       current_input=self.input_dim \n",
    "       \n",
    "       for i in range(self.N_cnn_lay):\n",
    "         \n",
    "         N_filt=int(self.cnn_N_filt[i])\n",
    "         len_filt=int(self.cnn_len_filt[i])\n",
    "         \n",
    "         # dropout\n",
    "         self.drop.append(nn.Dropout(p=self.cnn_drop[i]))\n",
    "         \n",
    "         # activation\n",
    "         self.act.append(act_fun(self.cnn_act[i]))\n",
    "                    \n",
    "         # layer norm initialization         \n",
    "         self.ln.append(LayerNorm([N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])]))\n",
    "\n",
    "         self.bn.append(nn.BatchNorm1d(N_filt,int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i]),momentum=0.05))\n",
    "            \n",
    "\n",
    "         if i==0:\n",
    "          self.conv.append(SincConv_fast(self.cnn_N_filt[0],self.cnn_len_filt[0],self.fs))\n",
    "              \n",
    "         else:\n",
    "          self.conv.append(nn.Conv1d(self.cnn_N_filt[i-1], self.cnn_N_filt[i], self.cnn_len_filt[i]))\n",
    "          \n",
    "         current_input=int((current_input-self.cnn_len_filt[i]+1)/self.cnn_max_pool_len[i])\n",
    "\n",
    "         \n",
    "       self.out_dim=current_input*N_filt\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "       batch=x.shape[0]\n",
    "       seq_len=x.shape[1]\n",
    "       \n",
    "       if bool(self.cnn_use_laynorm_inp):\n",
    "        x=self.ln0((x))\n",
    "        \n",
    "       if bool(self.cnn_use_batchnorm_inp):\n",
    "        x=self.bn0((x))\n",
    "        \n",
    "       x=x.view(batch,1,seq_len)\n",
    "\n",
    "       \n",
    "       for i in range(self.N_cnn_lay):\n",
    "           \n",
    "         if self.cnn_use_laynorm[i]:\n",
    "          if i==0:\n",
    "           x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(torch.abs(self.conv[i](x)), self.cnn_max_pool_len[i]))))  \n",
    "          else:\n",
    "           x = self.drop[i](self.act[i](self.ln[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))   \n",
    "          \n",
    "         if self.cnn_use_batchnorm[i]:\n",
    "          x = self.drop[i](self.act[i](self.bn[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i]))))\n",
    "\n",
    "         if self.cnn_use_batchnorm[i]==False and self.cnn_use_laynorm[i]==False:\n",
    "          x = self.drop[i](self.act[i](F.max_pool1d(self.conv[i](x), self.cnn_max_pool_len[i])))\n",
    "\n",
    "       \n",
    "       x = x.view(batch,-1)\n",
    "\n",
    "       return x\n",
    "   \n",
    "def str_to_bool(s):\n",
    "    if s == 'True':\n",
    "         return True\n",
    "    elif s == 'False':\n",
    "         return False\n",
    "    else:\n",
    "         raise ValueError \n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "# %%\n",
    "RESAMPLE_RATE = 16000\n",
    "\n",
    "fs=f\"{RESAMPLE_RATE}\"\n",
    "cw_len=\"1024\"\n",
    "cw_shift=\"10\"   \n",
    "\n",
    "cnn_N_filt=\"100,80,80\"\n",
    "cnn_len_filt=\"251,5,5\"\n",
    "cnn_max_pool_len=\"3,3,3\"\n",
    "cnn_use_laynorm_inp=\"True\"\n",
    "cnn_use_batchnorm_inp=\"False\"\n",
    "cnn_use_laynorm=\"True,True,True\"\n",
    "cnn_use_batchnorm=\"False,False,False\"\n",
    "cnn_act=\"leaky_relu,leaky_relu,leaky_relu\"\n",
    "cnn_drop=\"0.1,0.1,0.1\"\n",
    "\n",
    "\n",
    "fc_lay=\"2048,2048,2048\"\n",
    "fc_drop=\"0.1,0.1,0.1\"\n",
    "fc_use_laynorm_inp=\"True\"\n",
    "fc_use_batchnorm_inp=\"False\"\n",
    "fc_use_batchnorm=\"True,True,True\"\n",
    "fc_use_laynorm=\"False,False,False\"\n",
    "fc_act=\"leaky_relu,leaky_relu,leaky_relu\"\n",
    "\n",
    "class_lay=\"1\"\n",
    "class_drop=\"0.0\"\n",
    "class_use_laynorm_inp=\"False\"\n",
    "class_use_batchnorm_inp=\"False\"\n",
    "class_use_batchnorm=\"False\"\n",
    "class_use_laynorm=\"False\"\n",
    "class_act=\"softmax\"\n",
    "\n",
    "lr=\"0.0004\"\n",
    "batch_size=\"128\"\n",
    "N_epochs=\"1500\"\n",
    "N_batches=\"800\"\n",
    "N_eval_epoch=\"8\"\n",
    "seed=\"1234\"\n",
    "\n",
    "# %%\n",
    "cnn_N_filt=list(map(int, cnn_N_filt.split(',')))\n",
    "cnn_len_filt=list(map(int, cnn_len_filt.split(',')))\n",
    "cnn_max_pool_len=list(map(int, cnn_max_pool_len.split(',')))\n",
    "cnn_use_laynorm_inp=str_to_bool(cnn_use_laynorm_inp)\n",
    "cnn_use_batchnorm_inp=str_to_bool(cnn_use_batchnorm_inp)\n",
    "cnn_use_laynorm=list(map(str_to_bool, cnn_use_laynorm.split(',')))\n",
    "cnn_use_batchnorm=list(map(str_to_bool, cnn_use_batchnorm.split(',')))\n",
    "cnn_act=list(map(str, cnn_act.split(',')))\n",
    "cnn_drop=list(map(float, cnn_drop.split(',')))\n",
    "\n",
    "\n",
    "#[dnn]\n",
    "fc_lay=list(map(int, fc_lay.split(',')))\n",
    "fc_drop=list(map(float, fc_drop.split(',')))\n",
    "fc_use_laynorm_inp=str_to_bool(fc_use_laynorm_inp)\n",
    "fc_use_batchnorm_inp=str_to_bool(fc_use_batchnorm_inp)\n",
    "fc_use_batchnorm=list(map(str_to_bool, fc_use_batchnorm.split(',')))\n",
    "fc_use_laynorm=list(map(str_to_bool, fc_use_laynorm.split(',')))\n",
    "fc_act=list(map(str, fc_act.split(',')))\n",
    "\n",
    "#[class]\n",
    "class_lay=list(map(int, class_lay.split(',')))\n",
    "class_drop=list(map(float, class_drop.split(',')))\n",
    "class_use_laynorm_inp=str_to_bool(class_use_laynorm_inp)\n",
    "class_use_batchnorm_inp=str_to_bool(class_use_batchnorm_inp)\n",
    "class_use_batchnorm=list(map(str_to_bool, class_use_batchnorm.split(',')))\n",
    "class_use_laynorm=list(map(str_to_bool, class_use_laynorm.split(',')))\n",
    "class_act=list(map(str, class_act.split(',')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089a309d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desild/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU0 NVIDIA GB10 which is of cuda capability 12.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (8.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10960])\n",
      "10960\n",
      "torch.Size([2, 2048])\n",
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "wlen=4000\n",
    "\n",
    "\n",
    "# %%\n",
    "# Feature extractor CNN\n",
    "CNN_arch = {\n",
    "    'input_dim': wlen,\n",
    "    'fs': int(fs),\n",
    "    'cnn_N_filt': cnn_N_filt,\n",
    "    'cnn_len_filt': cnn_len_filt,\n",
    "    'cnn_max_pool_len':cnn_max_pool_len,\n",
    "    'cnn_use_laynorm_inp': cnn_use_laynorm_inp,\n",
    "    'cnn_use_batchnorm_inp': cnn_use_batchnorm_inp,\n",
    "    'cnn_use_laynorm':cnn_use_laynorm,\n",
    "    'cnn_use_batchnorm':cnn_use_batchnorm,\n",
    "    'cnn_act': cnn_act,\n",
    "    'cnn_drop':cnn_drop,          \n",
    "}\n",
    "\n",
    "\n",
    "CNN_net=SincNet(CNN_arch)\n",
    "CNN_net.cuda()\n",
    "\n",
    "\n",
    "\n",
    "DNN1_arch = {\n",
    "    'input_dim': CNN_net.out_dim,\n",
    "    'fc_lay': fc_lay,\n",
    "    'fc_drop': fc_drop, \n",
    "    'fc_use_batchnorm': fc_use_batchnorm,\n",
    "    'fc_use_laynorm': fc_use_laynorm,\n",
    "    'fc_use_laynorm_inp': fc_use_laynorm_inp,\n",
    "    'fc_use_batchnorm_inp':fc_use_batchnorm_inp,\n",
    "    'fc_act': fc_act,\n",
    "}\n",
    "\n",
    "DNN1_net=MLP(DNN1_arch)\n",
    "DNN1_net.cuda()\n",
    "\n",
    "\n",
    "DNN2_arch = {'input_dim':fc_lay[-1] ,\n",
    "          'fc_lay': class_lay,\n",
    "          'fc_drop': class_drop, \n",
    "          'fc_use_batchnorm': class_use_batchnorm,\n",
    "          'fc_use_laynorm': class_use_laynorm,\n",
    "          'fc_use_laynorm_inp': class_use_laynorm_inp,\n",
    "          'fc_use_batchnorm_inp':class_use_batchnorm_inp,\n",
    "          'fc_act': class_act,\n",
    "          }\n",
    "\n",
    "\n",
    "DNN2_net=MLP(DNN2_arch)\n",
    "DNN2_net.cuda()\n",
    "\n",
    "# %%\n",
    "inp = torch.randn(2,  wlen).cuda()\n",
    "out1 = CNN_net(inp)\n",
    "print(out1.shape)\n",
    "\n",
    "print(CNN_net.out_dim)\n",
    "out2 = DNN1_net(out1)\n",
    "print(out2.shape)\n",
    "\n",
    "# %%\n",
    "pout=DNN2_net(DNN1_net(CNN_net(inp)))\n",
    "print(pout.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5e198fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CNN_net_sr=SincNet(CNN_arch)\n",
    "CNN_net_sr.cuda()\n",
    "DNN1_net_sr=MLP(DNN1_arch)\n",
    "DNN1_net_sr.cuda()\n",
    "\n",
    "DNN2_arch_sr = {'input_dim':fc_lay[-1] ,\n",
    "          'fc_lay': [35],\n",
    "          'fc_drop': class_drop, \n",
    "          'fc_use_batchnorm': class_use_batchnorm,\n",
    "          'fc_use_laynorm': class_use_laynorm,\n",
    "          'fc_use_laynorm_inp': class_use_laynorm_inp,\n",
    "          'fc_use_batchnorm_inp':class_use_batchnorm_inp,\n",
    "          'fc_act': class_act,\n",
    "          }\n",
    "\n",
    "\n",
    "DNN2_net_sr=MLP(DNN2_arch_sr)\n",
    "DNN2_net_sr.cuda()\n",
    "\n",
    "sr_weight = torch.load(\"/home/desild/work/academic/sem3/TrustworthyML-assignment/tacotron2/vctk/models/SINCNET_SR/20251129_142613/checkpoint.pth\", weights_only=False)\n",
    "CNN_net_sr.load_state_dict(sr_weight['CNN_net'])\n",
    "DNN1_net_sr.load_state_dict(sr_weight['DNN1_net'])\n",
    "DNN2_net_sr.load_state_dict(sr_weight['DNN2_net'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2cdab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29664\n"
     ]
    }
   ],
   "source": [
    "from common_local.utils import load_wav_to_torch, load_filepaths_and_text, to_gpu\n",
    "from common_local.audio_processing import dynamic_range_compression, dynamic_range_decompression\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import librosa\n",
    "import random\n",
    "from typing import Optional, List, Tuple\n",
    "\n",
    "load_in_memory = {}\n",
    "\n",
    "class MelAudioLoaderVoxCeleb(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "        1) loads audio,text pairs\n",
    "        2) computes mel-spectrograms from audio files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, main_file, filter_length, hop_length, win_length,\n",
    "                 n_mel_channels, sampling_rate, mel_fmin, mel_fmax, segment_length, max_wav_value, base_directory='..',\n",
    "                 filter_speakers: Optional[List[str]] = None):\n",
    "        \n",
    "        self.main_file = main_file\n",
    "        if filter_speakers is not None:\n",
    "            self.main_file = self.main_file[self.main_file['speaker_id'].isin(filter_speakers)]\n",
    "            print(f\"Filtered dataset to {len(self.main_file)} items for speakers: {filter_speakers}\")\n",
    "\n",
    "        self.max_wav_value = max_wav_value\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.base_directory = base_directory\n",
    "        self.random_amp = 0.2\n",
    "        \n",
    "        self.filter_length = filter_length\n",
    "        self.hop_length = hop_length\n",
    "        self.win_length = win_length\n",
    "        self.n_mel_channels = n_mel_channels\n",
    "        self.mel_fmin = mel_fmin\n",
    "        self.mel_fmax = mel_fmax\n",
    "        self.stats = {\n",
    "            \"audio_len_distribution\": {},\n",
    "        }\n",
    "\n",
    "        \n",
    "        self.speaker_id2id_map = {}\n",
    "        self.segment_length = segment_length\n",
    "        random.seed(1234)\n",
    "        self.main_file = self.main_file.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "\n",
    "    def spectral_normalize(self, magnitudes):\n",
    "        output = dynamic_range_compression(magnitudes)\n",
    "        return output\n",
    "    \n",
    "    def load_text(self, file_path):\n",
    "        file_path = file_path.replace(\"WAV.wav\", \"WRD\")\n",
    "        with open(file_path, 'r') as f:\n",
    "            text = f.read().strip()\n",
    "            \n",
    "        text = text.split('\\n')\n",
    "        text = [x.strip().split(' ', 2) for x in text if len(x.split(' ', 2)) == 3]\n",
    "        return [int(x[0]) for x in text], [int(x[1]) for x in text], [x[2] for x in text]\n",
    "    \n",
    "    def get_mel_audio_pair(self, filename):\n",
    "        if filename not in load_in_memory:\n",
    "            audio, sampling_rate = librosa.load(filename, sr=None)\n",
    "            load_in_memory[filename] = {}\n",
    "            load_in_memory[filename][\"audio\"] = (audio, sampling_rate)\n",
    "        else:\n",
    "            audio, sampling_rate = load_in_memory[filename][\"audio\"]\n",
    "\n",
    "        # if sampling_rate != self.sampling_rate:\n",
    "        #     raise ValueError(\"{} {} SR doesn't match target {} SR\".format(\n",
    "        #         sampling_rate, self.sampling_rate))\n",
    "        \n",
    "        #\n",
    "        # if RESAMPLE_RATE != self.sampling_rate:\n",
    "        #import soundfile as sf\n",
    "        #sf.write(\"before_resample.wav\", audio, sampling_rate)\n",
    "        audio = librosa.resample(audio, orig_sr=sampling_rate, target_sr=config.sampling_rate)\n",
    "        #sf.write(\"after_resample.wav\", audio, sampling_rate//8)\n",
    "        audio = torch.from_numpy(audio)\n",
    "\n",
    "        self.stats[\"audio_len_distribution\"][filename] = audio.size(0)\n",
    "        # Take segment\n",
    "        if audio.size(0) >= self.segment_length:\n",
    "            max_audio_start = audio.size(0) - self.segment_length\n",
    "            audio_start = random.randint(0, max_audio_start)\n",
    "            audio = audio[audio_start:audio_start+self.segment_length]\n",
    "        else:\n",
    "            audio = torch.nn.functional.pad(\n",
    "                audio, (0, self.segment_length - audio.size(0)), 'constant').data\n",
    "        \n",
    "        #sf.write(\"after_resample2.wav\", audio.numpy(), sampling_rate//8)\n",
    "        audio = audio * random.uniform(1.0 - self.random_amp, 1.0 + self.random_amp)\n",
    "        audio_norm = audio / self.max_wav_value\n",
    "        mel_spec = librosa.feature.melspectrogram(\n",
    "            y=audio.numpy(),\n",
    "            sr=self.sampling_rate,\n",
    "            n_fft=self.filter_length,\n",
    "            hop_length=self.hop_length,\n",
    "            win_length=self.win_length,\n",
    "            n_mels=self.n_mel_channels,\n",
    "            fmin=self.mel_fmin,\n",
    "            fmax=self.mel_fmax\n",
    "        )\n",
    "        \n",
    "        mel_spec = torch.from_numpy(mel_spec).unsqueeze(0)\n",
    "        mel_output = self.spectral_normalize(mel_spec)\n",
    "        mel_output = mel_output.squeeze(0)\n",
    "        return mel_output, audio_norm, audio, len(audio)\n",
    "\n",
    "    def __getitem__(self, index) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, int]:\n",
    "        output = self.get_mel_audio_pair(self.main_file.iloc[index,2])\n",
    "\n",
    "        return output\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.main_file.shape[0]-1\n",
    "\n",
    "def batch_to_gpu(batch) -> Tuple[Tuple[torch.Tensor, torch.Tensor], torch.Tensor, torch.Tensor, List[str]]: \n",
    "    x, y, len_y, text = batch\n",
    "    x = to_gpu(x).float()\n",
    "    y = to_gpu(y).float()\n",
    "    len_y = to_gpu(torch.sum(len_y))\n",
    "\n",
    "    return (x, y), y, len_y, text\n",
    "\n",
    "def get_text_embedding(text, model):\n",
    "    embeddings = model.encode([text])\n",
    "    #print(embeddings.shape)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
    "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
    "    # Random weight term for interpolation between real and fake samples\n",
    "    if len(real_samples.shape) == 2:\n",
    "        alpha = torch.from_numpy(np.random.random((real_samples.size(0), 1)))\n",
    "    else:\n",
    "        alpha = torch.from_numpy(np.random.random((real_samples.size(0), 1, 1)))\n",
    "    alpha = alpha.to(real_samples.device).float()\n",
    "    # Get random interpolation between real and fake samples\n",
    "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
    "    #print(interpolates.shape, real_samples.shape, fake_samples.shape, alpha.shape)\n",
    "    d_interpolates = D(interpolates)\n",
    "    fake = Variable(torch.from_numpy(np.ones((real_samples.shape[0], 1))).float(), requires_grad=False)\n",
    "    # Get gradient w.r.t. interpolates\n",
    "    gradients = autograd.grad(\n",
    "        outputs=d_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=fake.to(real_samples.device),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True,\n",
    "    )[0]\n",
    "    gradients = gradients.view(gradients.size(0), -1)\n",
    "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
    "    return gradient_penalty\n",
    "\n",
    "# %%\n",
    "\n",
    "class Config:\n",
    "    # ** Audio params **\n",
    "    sampling_rate = 2000                        # Sampling rate\n",
    "    filter_length = 1024                         # Filter length\n",
    "    hop_length = 256                             # Hop (stride) length\n",
    "    win_length = 1024                            # Window length\n",
    "    mel_fmin = 0.0                               # Minimum mel frequency\n",
    "    mel_fmax = 8000.0                            # Maximum mel frequency\n",
    "    n_mel_channels = 80                          # Number of bins in mel-spectrograms\n",
    "    max_wav_value = 32768.0 \n",
    "    segment_length = 4000  \n",
    "    batch_size = 256  # Length of training segments (in samples)\n",
    "    epochs = 500\n",
    "    \n",
    "config = Config()\n",
    "\n",
    "train_data = pd.read_csv(\"/home/desild/work/academic/sem3/TrustworthyML-assignment/data/raw/vctk/unseen_data.csv\")\n",
    "\n",
    "# loss function\n",
    "cost = nn.NLLLoss()\n",
    "\n",
    "# %%\n",
    "print(len(train_data))\n",
    "\n",
    "# %%\n",
    "train_ds = MelAudioLoaderVoxCeleb(train_data, \n",
    "                                  config.filter_length, \n",
    "                                  config.hop_length, \n",
    "                                  config.win_length,\n",
    "                                  config.n_mel_channels, \n",
    "                                  config.sampling_rate, \n",
    "                                  config.mel_fmin, \n",
    "                                  config.mel_fmax,\n",
    "                                  config.segment_length, \n",
    "                                  config.max_wav_value\n",
    "                                  )\n",
    "\n",
    "train_loader = DataLoader(train_ds,\n",
    "                        num_workers=16,\n",
    "                        shuffle=True,\n",
    "                        batch_size=config.batch_size,\n",
    "                        drop_last=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e45cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c052cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00493637 -0.00512595 -0.00501381 ... -0.02475402 -0.02472734\n",
      "  0.01055151]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" >\n",
       "                    <source src=\"data:audio/wav;base64,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\" type=\"audio/wav\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8899) tensor(-11.5129) torch.Size([4000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "for d in train_ds:\n",
    "    mel, audio, orig_audio, len_audio = d\n",
    "    print(orig_audio.numpy())\n",
    "    plt.imshow(mel.numpy(), aspect='auto', origin='lower')\n",
    "    display(Audio(data=orig_audio.numpy(), rate=2000))\n",
    "    display(Audio(data=audio.numpy(), rate=2000))\n",
    "    print(mel.max(), mel.min(), audio.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "210eb6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated Diversity Loss (L_div): 5.9441\n",
      "Goal in Optimization: MAXIMIZE this value (e.g., minimize -L_div).\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class DiversityLoss(nn.Module):\n",
    "    def __init__(self, epsilon: float = 1e-8):\n",
    "        \"\"\"\n",
    "        Initializes the Diversity Loss module.\n",
    "        \n",
    "        Args:\n",
    "            epsilon: Small value added to the denominator to prevent division by zero.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon \n",
    "\n",
    "    def forward(self, features: torch.Tensor, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Calculates the feature space diversity loss (L_div) for a batch.\n",
    "\n",
    "        Args:\n",
    "            features (torch.Tensor): The generated feature vectors (F(G(Z))). Shape (B, D).\n",
    "            z (torch.Tensor): The latent input vectors (Z). Shape (B, L).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The mean ratio (L_div), which should be maximized.\n",
    "        \"\"\"\n",
    "        \n",
    "        # --- 1. Calculate Pairwise Distances ---\n",
    "        \n",
    "        # Numerator: Feature Distance Matrix (BxB)\n",
    "        # ||F(G(z1)) - F(G(z2))||\n",
    "        feature_dist_matrix = torch.cdist(features, features)\n",
    "        \n",
    "        # Denominator: Latent Distance Matrix (BxB)\n",
    "        # ||z1 - z2||\n",
    "        latent_dist_matrix = torch.cdist(z, z)\n",
    "\n",
    "        # --- 2. Compute the Ratio and Exclude Diagonal ---\n",
    "        \n",
    "        # Compute the ratio, adding epsilon to stabilize near-zero distances\n",
    "        ratio_matrix = feature_dist_matrix / (latent_dist_matrix + self.epsilon)\n",
    "\n",
    "        # The diagonal contains meaningless values (0/epsilon). We must exclude them.\n",
    "        \n",
    "        # Create a mask to exclude the diagonal elements (i=j)\n",
    "        mask = torch.ones_like(ratio_matrix, dtype=torch.bool)\n",
    "        mask.fill_diagonal_(False)\n",
    "        \n",
    "        # Apply the mask and calculate the mean of all non-diagonal ratios\n",
    "        diversity_loss = ratio_matrix[mask].mean()\n",
    "\n",
    "        return diversity_loss\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# 1. Setup\n",
    "BATCH_SIZE = 32\n",
    "LATENT_DIM = 10 \n",
    "FEATURE_DIM = 256\n",
    "\n",
    "# Instantiate the loss module\n",
    "l_div = DiversityLoss()\n",
    "\n",
    "# 2. Simulate Input Data (Tensors must be on the same device)\n",
    "Z_tensor = torch.randn(BATCH_SIZE, LATENT_DIM) # Input Z\n",
    "Feature_tensor = torch.randn(BATCH_SIZE, FEATURE_DIM) # Output F(G(Z))\n",
    "\n",
    "# 3. Calculate Loss\n",
    "loss_value = l_div(Feature_tensor, Z_tensor)\n",
    "\n",
    "print(f\"Calculated Diversity Loss (L_div): {loss_value.item():.4f}\")\n",
    "print(f\"Goal in Optimization: MAXIMIZE this value (e.g., minimize -L_div).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d4cc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SincNetCompound(\n",
       "  (cnn): SincNet(\n",
       "    (conv): ModuleList(\n",
       "      (0): SincConv_fast()\n",
       "      (1): Conv1d(100, 80, kernel_size=(5,), stride=(1,))\n",
       "      (2): Conv1d(80, 80, kernel_size=(5,), stride=(1,))\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(100, eps=1250, momentum=0.05, affine=True, track_running_stats=True)\n",
       "      (1): BatchNorm1d(80, eps=415, momentum=0.05, affine=True, track_running_stats=True)\n",
       "      (2): BatchNorm1d(80, eps=137, momentum=0.05, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (ln): ModuleList(\n",
       "      (0-2): 3 x LayerNorm()\n",
       "    )\n",
       "    (act): ModuleList(\n",
       "      (0-2): 3 x LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (drop): ModuleList(\n",
       "      (0-2): 3 x Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln0): LayerNorm()\n",
       "  )\n",
       "  (dnn1): MLP(\n",
       "    (wx): ModuleList(\n",
       "      (0): Linear(in_features=10960, out_features=2048, bias=True)\n",
       "      (1-2): 2 x Linear(in_features=2048, out_features=2048, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0-2): 3 x BatchNorm1d(2048, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (ln): ModuleList(\n",
       "      (0-2): 3 x LayerNorm()\n",
       "    )\n",
       "    (act): ModuleList(\n",
       "      (0-2): 3 x LeakyReLU(negative_slope=0.2)\n",
       "    )\n",
       "    (drop): ModuleList(\n",
       "      (0-2): 3 x Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (ln0): LayerNorm()\n",
       "  )\n",
       "  (dnn2): MLP(\n",
       "    (wx): ModuleList(\n",
       "      (0): Linear(in_features=2048, out_features=35, bias=True)\n",
       "    )\n",
       "    (bn): ModuleList(\n",
       "      (0): BatchNorm1d(35, eps=1e-05, momentum=0.05, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (ln): ModuleList(\n",
       "      (0): LayerNorm()\n",
       "    )\n",
       "    (act): ModuleList(\n",
       "      (0): LogSoftmax(dim=1)\n",
       "    )\n",
       "    (drop): ModuleList(\n",
       "      (0): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SincNetCompound(nn.Module):\n",
    "    def __init__(self, cnn, dnn1, dnn2):  \n",
    "        super(SincNetCompound, self).__init__()\n",
    "        self.cnn = cnn\n",
    "        self.dnn1 = dnn1\n",
    "        self.dnn2 = dnn2\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.dnn1(x)\n",
    "        x = self.dnn2(x)\n",
    "        return x\n",
    "    \n",
    "    def extract_features(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = self.dnn1(x)\n",
    "        return x\n",
    "    \n",
    "aud_disc = SincNetCompound(CNN_net, DNN1_net, DNN2_net)\n",
    "aud_disc.cuda()\n",
    "\n",
    "sr_model = SincNetCompound(CNN_net_sr, DNN1_net_sr, DNN2_net_sr)\n",
    "sr_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1837e3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.]], device='cuda:0', grad_fn=<LogSoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud_disc(torch.randn(2, wlen).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80777f5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generator(\n",
       "  (l1): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=320, bias=True)\n",
       "  )\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (1): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (4): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (5): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (11): Upsample(scale_factor=2.0, mode='nearest')\n",
       "    (12): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (15): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (16): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator = Generator()\n",
    "generator.apply(weights_init_normal)\n",
    "generator.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4507af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.nn import Conv1d, ConvTranspose1d, AvgPool1d, Conv2d\n",
    "from torch.nn.utils import weight_norm, remove_weight_norm, spectral_norm\n",
    "\n",
    "LRELU_SLOPE = 0.1\n",
    "\n",
    "def init_weights(m, mean=0.0, std=0.01):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        m.weight.data.normal_(mean, std)\n",
    "\n",
    "\n",
    "def apply_weight_norm(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        weight_norm(m)\n",
    "\n",
    "\n",
    "def get_padding(kernel_size, dilation=1):\n",
    "    return int((kernel_size*dilation - dilation)/2)\n",
    "\n",
    "class ResBlock1(torch.nn.Module):\n",
    "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3, 5)):\n",
    "        super(ResBlock1, self).__init__()\n",
    "        self.h = h\n",
    "        self.convs1 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[2],\n",
    "                               padding=get_padding(kernel_size, dilation[2])))\n",
    "        ])\n",
    "        self.convs1.apply(init_weights)\n",
    "\n",
    "        self.convs2 = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=1,\n",
    "                               padding=get_padding(kernel_size, 1)))\n",
    "        ])\n",
    "        self.convs2.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c1, c2 in zip(self.convs1, self.convs2):\n",
    "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            xt = c1(xt)\n",
    "            xt = F.leaky_relu(xt, LRELU_SLOPE)\n",
    "            xt = c2(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs1:\n",
    "            remove_weight_norm(l)\n",
    "        for l in self.convs2:\n",
    "            remove_weight_norm(l)\n",
    "\n",
    "\n",
    "class ResBlock2(torch.nn.Module):\n",
    "    def __init__(self, h, channels, kernel_size=3, dilation=(1, 3)):\n",
    "        super(ResBlock2, self).__init__()\n",
    "        self.h = h\n",
    "        self.convs = nn.ModuleList([\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[0],\n",
    "                               padding=get_padding(kernel_size, dilation[0]))),\n",
    "            weight_norm(Conv1d(channels, channels, kernel_size, 1, dilation=dilation[1],\n",
    "                               padding=get_padding(kernel_size, dilation[1])))\n",
    "        ])\n",
    "        self.convs.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for c in self.convs:\n",
    "            xt = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            xt = c(xt)\n",
    "            x = xt + x\n",
    "        return x\n",
    "\n",
    "    def remove_weight_norm(self):\n",
    "        for l in self.convs:\n",
    "            remove_weight_norm(l)\n",
    "\n",
    "\n",
    "class HIFIGenerator(torch.nn.Module):\n",
    "    def __init__(self, h):\n",
    "        super(HIFIGenerator, self).__init__()\n",
    "        self.h = h\n",
    "        self.num_kernels = len(h.resblock_kernel_sizes)\n",
    "        self.num_upsamples = len(h.upsample_rates)\n",
    "        self.conv_pre = weight_norm(Conv1d(80, h.upsample_initial_channel, 7, 1, padding=3))\n",
    "        resblock = ResBlock1 if h.resblock == '1' else ResBlock2\n",
    "\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i, (u, k) in enumerate(zip(h.upsample_rates, h.upsample_kernel_sizes)):\n",
    "            self.ups.append(weight_norm(\n",
    "                ConvTranspose1d(h.upsample_initial_channel//(2**i), h.upsample_initial_channel//(2**(i+1)),\n",
    "                                k, u, padding=(k-u)//2)))\n",
    "\n",
    "        self.resblocks = nn.ModuleList()\n",
    "        for i in range(len(self.ups)):\n",
    "            ch = h.upsample_initial_channel//(2**(i+1))\n",
    "            for j, (k, d) in enumerate(zip(h.resblock_kernel_sizes, h.resblock_dilation_sizes)):\n",
    "                self.resblocks.append(resblock(h, ch, k, d))\n",
    "\n",
    "        self.conv_post = weight_norm(Conv1d(ch, 1, 7, 1, padding=3))\n",
    "        self.ups.apply(init_weights)\n",
    "        self.conv_post.apply(init_weights)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_pre(x)\n",
    "        for i in range(self.num_upsamples):\n",
    "            x = F.leaky_relu(x, LRELU_SLOPE)\n",
    "            x = self.ups[i](x)\n",
    "            xs = None\n",
    "            for j in range(self.num_kernels):\n",
    "                if xs is None:\n",
    "                    xs = self.resblocks[i*self.num_kernels+j](x)\n",
    "                else:\n",
    "                    xs += self.resblocks[i*self.num_kernels+j](x)\n",
    "            x = xs / self.num_kernels\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv_post(x)\n",
    "        x = torch.tanh(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    \n",
    "hifigan_config = {\n",
    "    \"resblock\": \"1\",\n",
    "    \"num_gpus\": 0,\n",
    "    \"batch_size\": 16,\n",
    "    \"learning_rate\": 0.0002,\n",
    "    \"adam_b1\": 0.8,\n",
    "    \"adam_b2\": 0.99,\n",
    "    \"lr_decay\": 0.999,\n",
    "    \"seed\": 1234,\n",
    "\n",
    "    \"upsample_rates\": [8,8,2,2],\n",
    "    \"upsample_kernel_sizes\": [16,16,4,4],\n",
    "    \"upsample_initial_channel\": 512,\n",
    "    \"resblock_kernel_sizes\": [3,7,11],\n",
    "    \"resblock_dilation_sizes\": [[1,3,5], [1,3,5], [1,3,5]],\n",
    "\n",
    "    \"segment_size\": 8192,\n",
    "    \"num_mels\": 80,\n",
    "    \"num_freq\": 1025,\n",
    "    \"n_fft\": 1024,\n",
    "    \"hop_size\": 256,\n",
    "    \"win_size\": 1024,\n",
    "\n",
    "    \"sampling_rate\": 22050,\n",
    "\n",
    "    \"fmin\": 0,\n",
    "    \"fmax\": 8000,\n",
    "    \"fmax_for_loss\": None,\n",
    "    \"num_workers\": 4,\n",
    "\n",
    "    \"dist_config\": {\n",
    "        \"dist_backend\": \"nccl\",\n",
    "        \"dist_url\": \"tcp://localhost:54321\",\n",
    "        \"world_size\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "hifigan_config = dycomutils.config.ConfigDict(hifigan_config)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6df8bab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/desild/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/nn/utils/weight_norm.py:144: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HIFIGenerator(\n",
       "  (conv_pre): Conv1d(80, 512, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "  (ups): ModuleList(\n",
       "    (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "    (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))\n",
       "    (2): ConvTranspose1d(128, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "    (3): ConvTranspose1d(64, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
       "  )\n",
       "  (resblocks): ModuleList(\n",
       "    (0): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (1): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (2): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (3): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (4): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (5): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (6): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (7): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (8): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "    (9): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      )\n",
       "    )\n",
       "    (10): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       "      )\n",
       "    )\n",
       "    (11): ResBlock1(\n",
       "      (convs1): ModuleList(\n",
       "        (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))\n",
       "        (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))\n",
       "      )\n",
       "      (convs2): ModuleList(\n",
       "        (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (conv_post): Conv1d(32, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hifigan_weight = torch.load('/home/desild/work/academic/sem3/TrustworthyML-assignment/tacotron2/vctk/models/pretrain_hifigan/generator_v1')\n",
    "hifigan = HIFIGenerator(hifigan_config)\n",
    "hifigan.load_state_dict(hifigan_weight['generator'])\n",
    "hifigan.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "952883da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0802],\n",
       "        [-0.0714]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_disc = ImgDiscriminator()\n",
    "img_disc.cuda()\n",
    "img_disc(torch.randn(2,1,80,16).cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9123b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 80, 16])\n"
     ]
    }
   ],
   "source": [
    "mel = generator(torch.randn(1, opt.latent_dim).cuda())\n",
    "print(mel.shape)\n",
    "audios = hifigan(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e30afb65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 4096])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audios.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb6eb86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "optimizer_dis_aud = torch.optim.AdamW(aud_disc.parameters(), lr=float(lr), eps=1e-8, weight_decay=1e-4) \n",
    "optimizer_dis_img = torch.optim.AdamW(img_disc.parameters(), lr=float(lr), eps=1e-8, weight_decay=1e-4) \n",
    "optimizer_gen = torch.optim.AdamW(chain(generator.parameters(), hifigan.parameters()), lr=float(lr), eps=1e-8, weight_decay=1e-4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cbcbe9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 1/500 [06:57<57:51:53, 417.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     30\u001b[39m audio_validity = aud_disc(audios.squeeze(\u001b[32m1\u001b[39m))\n\u001b[32m     31\u001b[39m g_loss = -torch.mean(audio_validity) + -torch.mean(img_validity) - l_div(feat, z)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[43mg_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m optimizer_gen.step()\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m#  Train Audio Discriminator\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# ---------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/academic/sem3/TrustworthyML-assignment/.conda/lib/python3.11/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import Audio, display\n",
    "from tqdm import tqdm\n",
    "\n",
    "with tqdm(total=config.epochs, desc=\"Processing Batches\") as pbar:\n",
    "    for epoch in range(config.epochs):\n",
    "    # Used to calculate avg items/sec over epoch\n",
    "        epoch_logs = []\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            #(mel, aud), y, _, _, sp_id, _ = batch_to_gpu(batch)\n",
    "            true_mel, true_aud, orig_audio, _ = batch\n",
    "            true_mel = to_gpu(true_mel).float()\n",
    "            true_aud = to_gpu(true_aud).float()\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_gen.zero_grad()\n",
    "            # Sample noise as generator input\n",
    "            z = Variable(torch.randn(1, opt.latent_dim).cuda())\n",
    "            \n",
    "            mel = generator(torch.randn(true_aud.size(0), opt.latent_dim).cuda())\n",
    "            audios = hifigan(mel)\n",
    "            audios = audios[:,:, audios.shape[-1]//2 - true_aud.shape[-1]//2 : audios.shape[-1]//2 + true_aud.shape[-1]//2]\n",
    "            feat = sr_model.extract_features(audios.squeeze(1))\n",
    "            \n",
    "            #print(audios.shape, true_aud.shape)\n",
    "            # Calculate generator loss\n",
    "            img_validity = img_disc(mel.unsqueeze(1))\n",
    "            audio_validity = aud_disc(audios.squeeze(1))\n",
    "            g_loss = -torch.mean(audio_validity) + -torch.mean(img_validity) - l_div(feat, z)\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_gen.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Audio Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_dis_aud.zero_grad()\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            d_real = aud_disc(true_aud)\n",
    "            d_fake = aud_disc(audios.squeeze(1).detach())\n",
    "\n",
    "            gradient_penalty = compute_gradient_penalty(aud_disc, true_aud.data, audios.squeeze(1).data)\n",
    "            \n",
    "            # Adversarial loss\n",
    "            d_loss_aud = -torch.mean(d_real) + torch.mean(d_fake) + opt.lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss_aud.backward()\n",
    "            optimizer_dis_aud.step()\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Image Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_dis_img.zero_grad()\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            d_real = img_disc(true_mel.unsqueeze(1))\n",
    "            d_fake = img_disc(mel.unsqueeze(1).detach())\n",
    "\n",
    "            #print(true_mel.data.shape, mel.data.shape)\n",
    "            gradient_penalty = compute_gradient_penalty(img_disc, true_mel.data, mel.data)\n",
    "            \n",
    "            # Adversarial loss\n",
    "            d_loss_img = -torch.mean(d_real) + torch.mean(d_fake) + opt.lambda_gp * gradient_penalty\n",
    "\n",
    "            d_loss_img.backward()\n",
    "            optimizer_dis_img.step()\n",
    "            \n",
    "            epoch_logs.append({\n",
    "            \"d_mel_loss\": d_loss_img.item(),\n",
    "            \"d_aud_loss\": d_loss_aud.item(),\n",
    "            \"g_loss\": g_loss.item(),\n",
    "            })\n",
    "            \n",
    "            batches_done = epoch * len(train_ds) + i\n",
    "            if batches_done % opt.sample_interval == 0:\n",
    "                os.makedirs(\"images/HIFIGAN_ADV/\", exist_ok=True)\n",
    "                save_image(mel.unsqueeze(1)[:25], \"images/HIFIGAN_ADV/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "            \n",
    "            # print(\n",
    "            #     \"[Epoch %d/%d] [Batch %d/%d] [loss: %f] [error: %f]\"\n",
    "            #     % (epoch, config.epochs, i, len(train_loader), loss.item(), err.item())\n",
    "            # )\n",
    "            \n",
    "        \n",
    "        epoch_df = pd.DataFrame(epoch_logs)\n",
    "        run[\"train/epoch/d_mel_loss\"].append(epoch_df[\"d_mel_loss\"].mean())\n",
    "        run[\"train/epoch/d_aud_loss\"].append(epoch_df[\"d_aud_loss\"].mean())\n",
    "        run[\"train/epoch/g_loss\"].append(epoch_df[\"g_loss\"].mean())\n",
    "\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "# %%\n",
    "import datetime\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "model_type = \"HIFIGAN_ADV\"\n",
    "os.makedirs(\"models/{}/{}\".format(model_type, timestamp), exist_ok=True)\n",
    "\n",
    "config_dict = {\n",
    "    \"config\": config.__dict__,\n",
    "    \"model_config\": {\n",
    "        \"CNN_arch\": CNN_arch,\n",
    "        \"DNN1_arch\": DNN1_arch,\n",
    "        \"DNN2_arch\": DNN2_arch,\n",
    "        \"fs\": fs,\n",
    "        \"n_mel_channels\": config.n_mel_channels\n",
    "    }\n",
    "}\n",
    "\n",
    "save_dict = {\n",
    "    'dis_aud': aud_disc.state_dict(),\n",
    "    'dis_img': img_disc.state_dict(),\n",
    "    'gen': generator.state_dict(),\n",
    "    'hifigan': hifigan.state_dict(),\n",
    "    'optimizer_dis_aud': optimizer_dis_aud.state_dict(),\n",
    "    'optimizer_dis_img': optimizer_dis_img.state_dict(),\n",
    "    'optimizer_gen': optimizer_gen.state_dict(),\n",
    "    'epoch': config.epochs,\n",
    "    'batch_size': config.batch_size,\n",
    "    'learning_rate': float(lr),\n",
    "    'config': config_dict\n",
    "    }\n",
    "\n",
    "\n",
    "torch.save(save_dict, \"models/{}/{}/checkpoint.pth\".format(model_type, timestamp))\n",
    "# torch.save(model_waveglow.state_dict(), \"models/SINCNET/{}/waveglow.pth\".format(timestamp))\n",
    "# torch.save(audio_discriminator.state_dict(), \"models/SINCNET/{}/audio_discriminator.pth\".format(timestamp))\n",
    "\n",
    "run[\"model/saved_model/checkpoint\"].upload(\"models/{}/{}/checkpoint.pth\".format(model_type, timestamp))\n",
    "# run[\"model/saved_model/waveglow\"].upload(\"models/SINCNET/{}/waveglow.pth\".format(timestamp))\n",
    "# run[\"model/saved_model/audio_dis\"].upload(\"models/SINCNET/{}/audio_discriminator.pth\".format(timestamp))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e46d3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
